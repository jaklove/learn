1.进程的概念：
  我们编写的代码只是一个存储在硬盘的静态文件，通过编译后就会⽣成⼆进制可执⾏⽂件，当我们运⾏这个可执行文件后，它会被装载到内存中
,接着 CPU 会执行程序中的每条指令，那么这个运行中的程序，就被称为「进程」。

2.并发的概念：虽然单核的 CPU 在某个瞬间，只能运行1个进程。但在 1 秒钟期间，它可能会运行多个进程，这样就产生并发的错觉，实际上这是并发。

3.进程的状态？
  在一个进程的活动期间至少具备三种基本状态，即运行状态、就绪状态、阻塞状态。 
  .运行状态：该时刻进程占用cpu
  .就绪状态：可运行，由于其他进程处于运行状态而暂时停止运行
  .阻塞状态：该进程正在等待某一事件发生(如等待输入/输出操作的完成)而暂时停止运行，这时，即使给它cpu控制权，它也无法运行。
  
  
  进程的状态跃迁：
  1.创建状态：一个新进程被创建时的第一个状态
  2.创建状态 -> 就绪状态: 当进程被创建完并完成初始化后，一切就绪准备运行时,变为就绪状态,这个过程是很快的.
  3.就绪态 -> 运行状态：  处于就绪状态的进程被操作系统的进程调度器选中后，就分配给CPU正式运行该进程
  4.运行状态 -> 结束状态: 当进程已经运行完成或出错时，会被操作系统作结束状态处理
  5.运行状态 -> 就绪状态： 处于运行状态的进程在运行过程中，由于分配给它的运行时间片用完，操作系统会把进程变为就绪态，
    接着从就绪态选中另外一个进程运行
  6.运行状态 -> 阻塞状态：当进程请求某个事件且必须等待时，例如请求 I/O 事件	
  7.阻塞状态 -> 就绪状态：当进程要等待的事件完成时，它从阻塞状态变到就绪状态； 

  如果果有大量处于阻塞状态的进程，进程可能会占着物理内存空间，显然不是我们所希望的，毕竟物理内存空间是有限的，
被阻塞状态的进程占着物理内存就一种浪费物理内存的行为，所以，在虚拟内存管理的操作系统中，通常会把阻塞状态的进程的物理内存空间换出到硬盘，
等需要再次运行的时候，再从硬盘换入到物理内存，那么就需要一个新的状态来描述进程没有占用实际的物理内存空间的情况，这个状态就是挂起状态。这跟
阻塞状态是不一样，阻塞状态是等待某个事件的返回。
  
  挂起状态可以分为两种：
   .阻塞挂起状态:进程在外存(硬盘)并等待某个事件的出现
   .就绪挂起状态:进程在外存(硬盘),但只要进入内存，即刻立刻运行；
   
  导致进程挂起的原因不只是因为进程所使用的内存空间不在物理内存，还包括如下情况：
   1.通过 sleep 让进程间歇性挂起，其⼯作原理是设置一个定时器，到期后唤醒进程.
   2.用户希望挂起⼀个程序的执行，比如在 Linux 中用 Ctrl+Z 挂起进程；
   
   
  在操作系统中，是用进程控制块（process control block，PCB）数据结构来描述进程的，PCB 是进程存在的唯一标识，这意味着1个进程的存在，必然会有一个 PCB，如果进程消失
了，那么 PCB 也会随之消失。
   PCB包含进程标识符(标识各个进程，每个进程都有1个并且唯一的标识符)、用户标识符(进程归属的用户，用户标识符主要为共享和保护服务)
   
   进程控制和管理信息：
    1.进程当前状态
	2.进程优先级：进程抢占 CPU 时的优先级；
	
   资源分配清单:
    1.有关内存地址空间或虚拟地址空间的信息，所打开文件的列表和所使用的 I/O 设备信息。   
    
   CPU 相关信息：
    CPU 中各个寄存器的值，当进程被切换时，CPU 的状态信息都会被保存在相应的 PCB中，以便进程重新执行时，能从断点处继续执行
	
每个pcb如何组织的？
   1.通常是通过链表的方式进行组织，把具有相同状态的进程链在一起，组成各种队列，将所有处于就绪状态的进程链在一起，称为就绪队列，把所有因等待某事件而处于等待状态的进程链在一起就组成各种阻塞队列。	
   2.另外，对于运行队列在单核 CPU 系统中则只有1个运行指针了，因为单核 CPU 在某个时间，只能运行一个程序。
   
   
创建进程相关知识？
   操作系统允许一个进程创建另一个进程，而且允许子进程继承父进程所拥有的资源，当子进程被终止时，其在父进程处继承的资源应当还给父进程。同时，终止父进程时同时也会终止
其所有的子进程。
   
 
创建进程:
  1.为新进程分配一个唯一的进程标识号,并申请一个空白的PCB,PCB是有限的,若申请失败则创建失败； 
  2.为进程分配资源，此处如果资源不足，进程就会进入等待状态，以等待资源。
  3.初始化PCB；
  4.如果进程的调度队列能够接纳新进程，那就将进程插⼊到就绪队列，等待被调度运行。
  
终止进程:
  进程可以有 3 种终止方式：正常结束、异常结束以及外界干预（信号 kill 掉）。
  1.查找需要终⽌的进程的 PCB；
  2.如果处于执行状态，则立即终止该进程的执行，然后将 CPU 资源分配给其他进程  
  3.如果其还有子进程，则应将其所有子进程终止
  4.将该进程所拥有的全部资源都归还给父进程或操作系统 
  5.将其从PCB所在队列中删除   
  
  
阻塞进程
  当进程需要等待某⼀事件完成时，它可以调用阻塞语句自己阻塞等待。而一旦被阻塞等
待，它只能由另1个进程唤醒。  
  阻塞进程的过程如下：
    1.找到将要被阻塞进程标识号对应的 PCB
	2.如果该进程为运行状态，则保护其现场，将其状态转为阻塞状态，停止运行
	3.将该PCB插到到阻塞队列中去；
  
唤醒进程  
    1.运行」转变为「阻塞」状态是由于进程必须等待某件事件的完成，所以处于阻塞状态的进程是绝对不可能叫醒自己的.
	2.如果某进程正在等待 I/O 事件，需由别的进程发消息给它，则只有当该进程所期待的事件出
现时，才由发现者进程用唤醒语句叫醒它
    
唤醒进程的过程如下：
   1.在该事件的阻塞队列中找到相应进程的PCB
   2.将其从阻塞队列中移出，并置其状态为就绪状态
   3.把该 PCB 插⼊到就绪队列中，等待调度程序调度；
     
进程的阻塞和唤醒是⼀对功能相反的语句，如果某个进程调⽤了阻塞语句，则必有⼀个与之
对应的唤醒语句。

进程的上下文切换
   1.各个进程之间是共享 CPU 资源的，在不同的时候进程之间需要切换，让不同的进程可以在
CPU执行，那么这个一个进程切换到另一个进程运行，称为进程的上下文切换。	 

CPU上下文切换：
   CPU 寄存器是 CPU 内部一个容量小，但是速度极快的内存(缓存)。再来，程序计数器则是⽤来存储 CPU 正在执⾏的指令位置、或者即将执行的下一条指令位置，程序计数器则是用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位
置,所以说，CPU 寄存器和程序计数是 CPU 在运行任何任务前所必须依赖的环境，这些环境就叫做 CPU上下文。CPU 上下文切换就是先把前一个任务的 CPU 上下文（CPU 寄存器和程序计数器）保存起
来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。系统内核会存储保持下来的上下文信息，当此任务再次被分配给 CPU运行时，CPU 会重新
加载这些上下文，这样就能保证任务原来的状态不受影响，让任务看起来还是连续。


进程的上下文切换到底是切换什么呢？
  进程的上下文切换不仅包含了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。通常，会把交换的信息保存在进程的 PCB，当要运行另外1个进程的时候，我们需要从这个
进程的 PCB 取出上下文，然后恢复到 CPU 中，这使得这个进程可以继续执行，


发生进程上下文切换有哪些场景？   
   1.为了保证所有进程可以得到公平调度，CPU 时间被划分为⼀段段的时间⽚，这些时间片
再被轮流分配给各个进程。这样，当某个进程的时间⽚耗尽了，进程就从运行状态变为就
绪状态，系统从就绪队列选择另外一个进程运行；
  2.进程在系统资源不足（如内存不足）时，要等到资源满足后才可以运行，这个时候进程
也会被挂起，并由系统调度其他进程运行
  3.当进程通过睡眠函数sleep这样的方法将自己主动挂起时，自然也会重新调度；
  4.当有优先级更⾼的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行。
  5.发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序


1.线程
   在早期的操作系统中都是以进程作为独⽴运⾏的基本单位，直到后⾯，计算机科学家们⼜提出了更⼩的能独⽴运⾏的基本单位，也就是线程。
直白点线程是进程当中的⼀条执⾏流程，同⼀个进程内多个线程之间可以共享代码段、数据段、打开的⽂件等资源，但每个线程各⾃都有⼀套独⽴的寄存器和栈，
这样可以确保线程的控制流是相对独⽴的。

  线程有哪些优点：
    1.一个进程中可以同时存在多个线程；
    2.各个线程之间可以并发执⾏；
    3.各个线程之间可以共享地址空间和⽂件等资源；

  线程有哪些缺点：
    1.当进程中的⼀个线程崩溃时，会导致其所属进程的所有线程崩溃


  线程与进程的⽐较如下：
    1.进程是资源（包括内存、打开的⽂件等）分配的单位，线程是 CPU 调度的单位；
    2.进程拥有⼀个完整的资源平台，⽽线程只独享必不可少的资源，如寄存器和栈；
    3.线程同样具有就绪、阻塞、执⾏三种基本状态，同样具有状态之间的转换关系；
    4.线程能减少并发执⾏的时间和空间开销；

  对于，线程相⽐进程能减少开销，体现在:
    1.线程的创建时间⽐进程快，因为进程在创建的过程中，还需要资源管理信息，⽐如内存管理信息、⽂件管理信息，⽽线程在创建的过程中，不会涉及这些资源管理信息，⽽是共享它们.
    2.线程的终⽌时间⽐进程快，因为线程释放的资源相⽐进程少很多;
    3.同⼀个进程内的线程切换⽐进程切换快，因为线程具有相同的地址空间（虚拟内存共
      享），这意味着同⼀个进程的线程都具有同⼀个⻚表，那么在切换的时候不需要切换⻚
      表。⽽对于进程之间的切换，切换的时候要把⻚表给切换掉，⽽⻚表的切换过程开销是⽐较⼤的
    4.于同⼀进程的各线程间共享内存和⽂件资源，那么在线程之间数据传递的时候，就不需要经过内核了，这就使得线程之间的数据交互效率更⾼了；

线程的上下⽂切换?
    在前⾯我们知道了，线程与进程最⼤的区别在于：线程是调度的基本单位，⽽进程则是资源拥有的基本单位,所以，所谓操作系统的任务调度，实际上的调度对象是线程，
⽽进程只是给线程提供了虚拟内存、全局变量等资源。

对于线程和进程，我们可以这么理解：
    1.当进程只有⼀个线程时，可以认为进程就等于线程；
    2.当进程拥有多个线程时，这些线程会共享相同的虚拟内存和全局变量等资源，这些资源在上下⽂切换时是不需要修改的；
    3.另外，线程也有⾃⼰的私有数据，⽐如栈和寄存器等，这些在上下⽂切换时也是需要保存的

这还得看线程是不是属于同⼀个进程:
     1.当两个线程不是属于同⼀个进程，则切换的过程就跟进程上下⽂切换⼀样；
     2.当两个线程是属于同⼀个进程，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据；
     3.因此线程的上下文切换相比进程开销要小很多。

线程的实现
     1.⽤户线程（User Thread）：在⽤户空间实现的线程，不是由内核管理的线程，是由⽤户态的线程库来完成线程的管理；
     2.内核线程（Kernel Thread）：在内核中实现的线
     3.轻量级进程（LightWeight Process）：在内核中来⽀持⽤户线程；

⽤户线程如何理解？存在什么优势和缺陷？
     ⽤户线程是基于⽤户态的线程管理库来实现的，那么线程控制块（Thread Control Block,TCB）也是在库⾥⾯来实现的，对于操作系统⽽⾔是看不到这个 TCB 的，它只能看到整个进程的 PCB。
所以，⽤户线程的整个线程管理和调度，操作系统是不直接参与的，⽽是由⽤户级线程库函数来完成线程的管理，包括线程的创建、终⽌、同步和调度等。


⽤户线程的优点：
     1.每个进程都需要有它私有的线程控制块（TCB）列表，⽤来跟踪记录它各个线程状态信息（PC、栈指针、寄存器），TCB 由⽤户级线程库函数来维护，可⽤于不⽀持线程技术的操作系统；
     2.⽤户线程的切换也是由线程库函数来完成的，⽆需⽤户态与内核态的切换，所以速度特别快；

⽤户线程的缺点：
    1.由于操作系统不参与线程的调度，如果⼀个线程发起了系统调⽤⽽阻塞，那进程所包含的⽤户线程都不能执⾏了。
    2.当⼀个线程开始运⾏后，除⾮它主动地交出 CPU 的使⽤权，否则它所在的进程当中的其他线程⽆法运⾏，因为⽤户态的线程没法打断当前运⾏中的线程，
      它没有这个特权，只有操作系统才有，但是⽤户线程不是由操作系统管理的。
    3.由于时间⽚分配给进程，故与其他进程⽐，在多线程执⾏时，每个线程得到的时间⽚较少，执⾏会⽐较慢；

内核线程是由操作系统管理的，线程对应的 TCB ⾃然是放在操作系统⾥的，这样线程的创建、终⽌和管理都是由操作系统负责。

内核线程的优点：
   1.在⼀个进程当中，如果某个内核线程发起系统调⽤⽽被阻塞，并不会影响其他内核线程的运⾏；
   2.分配给线程，多线程的进程获得更多的 CPU 运⾏时间；

内核线程的缺点：
   1.在⽀持内核线程的操作系统中，由内核来维护进程和线程的上下⽂信息，如PCB和TCB；
   2.线程的创建、终⽌和切换都是通过系统调⽤的⽅式来进⾏，因此对于系统来说，系统开销⽐较⼤；

调度原则：
  原则⼀：如果运⾏的程序，发⽣了 I/O 事件的请求，那 CPU 使⽤率必然会很低，因为此时进程在阻塞等待硬盘的数据返回，
这样的过程，势必会造成 CPU 突然的空闲。所以，为了提⾼CPU 利⽤率，在这种发送 I/O 事件致使 CPU 空闲的情况下，调度程序需要从就绪队列中选择⼀个进程来运⾏。
  原则⼆：有的程序执⾏某个任务花费的时间会⽐较⻓，如果这个程序⼀直占⽤着 CPU，会造成系统吞吐量（CPU 在单位时间内完成的进程数量）的降低。
所以，要提⾼系统的吞吐率，调度程序要权衡⻓任务和短任务进程的运⾏完成数量。
  原则三：从进程开始到结束的过程中，实际上是包含两个时间，分别是进程运⾏时间和进程等待时间，这两个时间总和就称为周转时间。进程的周转时间越⼩越好，
如果进程的等待时间很⻓⽽运⾏时间很短，那周转时间就很⻓，这不是我们所期望的，调度程序应该避免这种情况发⽣。
  原则四：处于就绪队列的进程，也不能等太久，当然希望这个等待的时间越短越好，这样可以使得进程更快的在 CPU 中执⾏。
所以，就绪队列中进程的等待时间也是调度程序所需要考虑的原则。
  原则五：对于⿏标、键盘这种交互式⽐较强的应⽤，我们当然希望它的响应时间越快越好，否则就会影响⽤户体验了。
所以，对于交互式⽐较强的应⽤，响应时间也是调度程序需要考虑的原则。


根据上面调度的原则，总结如下:
  CPU 利⽤率：调度程序应确保 CPU 是始终匆忙的状态，这可提⾼ CPU 的利⽤率；
  系统吞吐量：吞吐量表示的是单位时间内 CPU 完成进程的数量，⻓作业的进程会占⽤较⻓的 CPU 资源，因此会降低吞吐量，相反，短作业的进程会提升系统吞吐量；
  周转时间：周转时间是进程运⾏和阻塞时间总和，⼀个进程的周转时间越⼩越好；
  等待时间：这个等待时间不是阻塞状态的时间，⽽是进程处于就绪队列的时间，等待的时间越⻓，⽤户越不满意；
  响应时间：⽤户提交请求到系统第⼀次产⽣响应所花费的时间，在交互式系统中，响应时间是衡量调度算法好坏的主要标准。

调度算法
  1.最简单的⼀个调度算法，就是⾮抢占式的先来先服务（First Come First Seved, FCFS）算法了，每次从就绪队列选择最先进⼊队列的进程，
     然后⼀直运⾏，直到进程退出或被阻塞，才会继续从队列中选择第⼀个进程接着运⾏。
  2.最短作业优先（Shortest Job First, SJF）调度算法同样也是顾名思义，它会优先选择运⾏时间最短的进程来运⾏，这有助于提⾼系统的吞吐量。
  3.⾼响应⽐优先（Highest Response Ratio Next, HRRN）调度算法主要是权衡了短作业和⻓作业。(优先权=(等待时间+要求服务时间)/要求服务时间)
  4.最古⽼、最简单、最公平且使⽤最⼴的算法就是时间⽚轮转（Round Robin, RR）调度算法。(每个进程被分配⼀个时间段，称为时间⽚（Quantum），即允许该进程在该时间段中运⾏。)
  5.希望调度程序能从就绪队列中选择最⾼优先级的进程进⾏运⾏，这称为最⾼优先级（Highest Priority First，HPF）调度算法。
    进程的优先级可以分为，静态优先级和动态优先级：
    1.静态优先级：创建进程时候，就已经确定了优先级了，然后整个运⾏时间优先级都不会变化
    2.动态优先级：根据进程的动态变化调整优先级，⽐如如果进程运⾏时间增加，则降低其优先级，如果进程等待时间（就绪队列的等待时间）增加，则升⾼其优先级，也就是随着时间的推移增加等待进程的优先级

    该算法也有两种处理优先级⾼的⽅法，⾮抢占式和抢占式：
      1.⾮抢占式：当就绪队列中出现优先级⾼的进程，运⾏完当前进程，再选择优先级⾼的进程。
      2.抢占式：当就绪队列中出现优先级⾼的进程，当前进程挂起，调度优先级⾼的进程运⾏。
      3.但是依然有缺点，可能会导致低优先级的进程永远不会运⾏

  6.多级反馈队列（Multilevel Feedback Queue）调度算法是「时间⽚轮转算法」和「最⾼优先级算法」的综合和发展。
     「多级」表示有多个队列，每个队列优先级从⾼到低，同时优先级越⾼时间⽚越短。
     「反馈」表示如果有新的进程加⼊优先级⾼的队列时，⽴刻停⽌当前正在运⾏的进程，转⽽去运⾏优先级⾼的队列
    如何工作：
      1.设置了多个队列，赋予每个队列不同的优先级，每个队列优先级从⾼到低，同时优先级越⾼时间⽚越短；
      2.新的进程会被放⼊到第⼀级队列的末尾，按先来先服务的原则排队等待被调度，如果在第⼀级队列规定的时间⽚没运⾏完成，则将其转⼊到第⼆级队列的末尾，以此类推，直⾄完成
      3.当较⾼优先级的队列为空，才调度较低优先级的队列中的进程运⾏。如果进程运⾏时，有新进程进⼊较⾼优先级的队列，则停⽌当前运⾏的进程并将其移⼊到原队列末尾，接着让较⾼优先级的进程运⾏；

进程间通信？
  管道、消息队列、共享内存、信号量、信号、socket
  1.通信的⽅式是单向的，数据只能在⼀个⽅向上流动，如果要双向通信，需要创建两个管道，再来匿名管道是只能⽤于存在⽗⼦关系的进程间通信，匿名管道的⽣命周期随着进程创建⽽建⽴，随着进程终⽌⽽消失。
命名管道突破了匿名管道只能在亲缘关系进程间的通信限制，因为使⽤命名管道的前提，需要在⽂件系统创建⼀个类型为 p 的设备⽂件，那么毫⽆关系的进程就可以通过这个设备⽂件进⾏通信。另外，不管是匿名管道还是命名管道，进程写⼊的数据都是缓存在内核中，另⼀个进程读取数据时候⾃然也是从内核中获取，同时通信数据都遵循先进先出原则，不⽀持
lseek 之类的⽂件定位操作。
  2.消息队列克服了管道通信的数据是⽆格式的字节流的问题，消息队列实际上是保存在内核的「消息链表」，消息队列的消息体是可以⽤户⾃定义的数据类型，发送数据时，会被分成⼀个⼀个独⽴的消息体，当然接收数据时，也要与发送⽅发送的消息体的数据类型保持⼀致，这样才能保证读取的数据是正确的。
消息队列通信的速度不是最及时的，毕竟每次数据的写⼊和读取都需要经过⽤户态与内核态之间的拷⻉过程.
  3.共享内存可以解决消息队列通信中⽤户态与内核态之间数据拷⻉过程带来的开销，它直接分配⼀个共享空间，每个进程都可以直接访问，就像访问进程⾃⼰的空间⼀样快捷⽅便，不需要陷⼊内核态或者系统调⽤，⼤⼤提⾼了通信的速度，享有最快的进程间通信⽅式之名。但是便捷⾼效的共享内存通信，带来新的问题，
多进程竞争同个共享资源会造成数据的错乱。
  4.⽅式就是互斥访问。信号量不仅可以实现访问的互斥性，还可以实现进程间的同步，信号量其实是⼀个计数器，表示的是资源个数，其值可以通过两个原⼦操作来控制，分别是P 操作和 V 操作。
  5.与信号量名字很相似的叫信号，它俩名字虽然相似，但功能⼀点⼉都不⼀样。信号是进程间通信机制中唯⼀的异步通信机制，信号可以在应⽤进程和内核之间直接交互，内核也可以利⽤信号来通知⽤户空间的进程发⽣了哪些系统事件，
信号事件的来源主要有硬件来源（如键盘 Cltr+C ）和软件来源（如 kill 命令），⼀旦有信号发⽣，进程有三种⽅式响应信号 1. 执⾏默认操作、2. 捕捉信号、3. 忽略信号。有两个信号是应⽤进程⽆法捕捉和忽略的，即
SIGKILL和SEGSTOP，这是为了⽅便我们能在任何时候结束或停⽌某个进程。


互斥和同步的概念：
   互斥（mutualexclusion)的，也就说保证⼀个线程在临界区执⾏时，其他线程应该被阻⽌进⼊临界区，说⽩了，就是这段代码执⾏过程中，最多只能出现⼀个线程。
   同步：所谓同步，就是并发进程/线程在⼀些关键点上可能需要互相等待与互通消息，这种相互制约的等待与互通信息称为进程/线程同步。

    操作系统必须提供实现进程协作的措施和⽅法，主要的⽅法有两种：1.锁：加锁、解锁操作； 2.信号量：P、V 操作；
这两个都可以⽅便地实现进程/线程互斥，⽽信号量⽐锁的功能更强⼀些，它还可以⽅便地实
现进程/线程同步。

锁？
  任何想进⼊临界区的线程，必须先执⾏加锁操作。若加锁操作顺利通过，则线程可进⼊临界区；在完成对临界资源的访问后再执⾏解锁操作，以释放该临界资源。

互斥锁和自旋锁的区别？
    互斥锁和自旋锁是最底层的两种锁，其他的很多锁都是基于他们的实现。当线程A获取到锁后，线程B再去获取锁，有两种处理方式，第一种是线程B循环的去尝试获取锁，
 直到获取成功为止即自旋锁，另一种是线程B放弃获取锁，在锁空闲时，等待被唤醒，即互斥锁。
	互斥锁加锁流程：
	   (1) 当线程加锁失败，内核会把线程的状态由“运行”设置为“睡眠”，让出cpu；
	   (2) 当锁空闲时，内核唤醒线程，状态设置为“就绪”，获取cpu执行；

  而自旋锁会自用户态由应用程序完成，不涉及用户态到内核态的转化，没有线程上下文切换，性能相对较好。自旋锁加锁过程：
     自旋锁加锁流程：
        (1) 查看锁的状态；
        (2) 锁空闲，获取锁，否则执行(1)；

   自旋锁会利用cpu一直工作直到获取到锁，中间不会释放cpu，但如果被锁住的代码执行时间较长，导致cpu空转，浪费资源。

两种锁适用于不同场景：
     1.如果是多核处理器，如果预计线程等待锁的时间很短，短到比线程两次上下文切换时间要少的情况下，使用自旋锁是划算的。
     2.如果是多核处理器，如果预计线程等待锁的时间较长，至少比两次线程上下文切换的时间要长，建议使用互斥量。
     3.如果是单核处理器，一般建议不要使用自旋锁。因为，在同一时间只有一个线程是处在运行状态，那如果运行线程发现无法获取锁，
       只能等待解锁，但因为自身不挂起，所以那个获取到锁的线程没有办法进入运行状态，只能等到运行线程把操作系统分给它的时间片用完，
       才能有机会被调度。这种情况下使用自旋锁的代价很高。

信号量
   信号量是操作系统提供的⼀种协调共享资源访问的⽅法,通常信号量表示资源的数量，对应的变量是⼀个整型（ sem ）变量
另外，还有两个原⼦操作的系统调⽤函数来控制信号量的，分别是：
  1.P 操作：将 sem 减 1 ，相减后，如果 sem < 0 ，则进程/线程进⼊阻塞等待，否则继 续，表明 P 操作可能会阻塞；
  2.V 操作：将 sem 加 1 ，相加后，如果 sem <= 0 ，唤醒⼀个等待中的进程/线程，表明V 操作不会阻塞

信号量实现临界区的互斥访问?
   任何想进⼊临界区的线程，必先在互斥信号量上执⾏ P 操作，在完成对临界资源的访问后再执⾏ V 操作。由于互斥信号量的初始值为 1，
故在第⼀个线程执⾏ P 操作后 s 值变为0，表示临界资源为空闲，可分配给该线程，使之进⼊临界区.若此时⼜有第⼆个线程想进⼊临界区，
也应先执⾏ P 操作，结果使 s 变为负值，这就意味着临界资源已被占⽤，因此，第⼆个线程被阻塞。
并且，直到第⼀个线程执⾏ V 操作，释放临界资源⽽恢复 s 值为 0 后，才唤醒第⼆个线程，使之进⼊临界区，待它完成临界资源的访问后，
⼜执⾏ V 操作，使 s 恢复到初始值 1。

对于两个并发线程，互斥信号量的值仅取 1、0 和 -1 三个值，分别表示：
  1.如果互斥信号量为 1，表示没有线程进⼊临界区；
  2.如果互斥信号量为 0，表示有⼀个线程进⼊临界区；
  3.如果互斥信号量为 -1，表示⼀个线程进⼊临界区，另⼀个线程等待进⼊。
  通过互斥信号量的⽅式，就能保证临界区任何时刻只有⼀个线程在执⾏，就达到了互斥的效果。

信号量实现事件同步: 同步的⽅式是设置⼀个信号量，其初值为 0
  1.妈妈⼀开始询问⼉⼦要不要做饭时，执⾏的是 P(s1) ，相当于询问⼉⼦需不需要吃饭，由于s1 初始值为 0，此时 s1 变成 -1，
表明⼉⼦不需要吃饭，所以妈妈线程就进⼊等待状态。
  2.当⼉⼦肚⼦饿时，执⾏了 V(s1) ，使得 s1 信号量从 -1 变成 0，表明此时⼉⼦需要吃饭了，于是就唤醒了阻塞中的妈妈线程，妈妈线程就开始做饭
  3.接着，⼉⼦线程执⾏了 P(s2) ，相当于询问妈妈饭做完了吗，由于 s2 初始值是 0，则此时s2 变成 -1，说明妈妈还没做完饭，⼉⼦线程就等待状态
  4.最后，妈妈终于做完饭了，于是执⾏ V(s2) ， s2 信号量从 -1 变回了 0，于是就唤醒等待中的⼉⼦线程，唤醒后，⼉⼦线程就可以进⾏吃饭了。

死锁
   1.死锁的概念；
   2.模拟死锁问题的产⽣；
   3.利⽤⼯具排查死锁问题；
   4.避免死锁问题的发⽣

死锁的产生: 当两个线程为了保护两个不同的共享资源⽽使⽤了两个互斥锁，那么这两个互斥锁应⽤不当的时候，可能会造成两个线程都在等待对⽅释放锁，
在没有外⼒的作⽤下，这些线程会⼀直相互等待，就没办法继续运⾏，这种情况就是发⽣了死锁。

死锁只有同时满⾜以下四个条件才会发⽣：
  1.互斥条件 (互斥条件是指多个线程不能同时使⽤同⼀个资源)
  2.持有并等待条件 (当线程 A 已经持有了资源 1，⼜想申请资源 2，⽽资源 2 已经被线程C 持有了，
                    所以线程 A 就会处于等待状态，但是线程 A 在等待资源 2 的同时并不会释放⾃⼰已经持有的资源 1。)
  3.不可剥夺条件
  4.环路等待条件   (在死锁发⽣的时候，两个线程获取资源的顺序构成了环形链)
                    :线程 A 已经持有资源 2，⽽想请求资源 1， 线程 B 已经获取了资源 1，⽽想请求资源2，这就形成资源请求等待的环形图。


悲观锁与乐观锁

互斥锁与⾃旋锁：谁更轻松⾃如？
    当已经有⼀个线程加锁后，其他线程加锁则就会失败，互斥锁和⾃旋锁对于加锁失败后的处理⽅式是不⼀样的：
    1.互斥锁加锁失败后，线程会释放 CPU ，给其他线程。
    2.⾃旋锁加锁失败后，线程会忙等待，直到它拿到锁。

互斥锁是⼀种「独占锁」，⽐如当线程 A 加锁成功后，此时互斥锁已经被线程 A 独占了，只
    要线程 A 没有释放⼿中的锁，线程 B 加锁就会失败，于是就会释放 CPU 让给其他线程，既
    然线程 B 释放掉了 CPU，⾃然线程 B 加锁的代码就会被阻塞。(对于互斥锁加锁失败⽽阻塞的现象，是由操作系统内核实现的),
    。当加锁失败时，内核会将线程置为「睡眠」状态，等到锁被释放后，内核会在合适的时机唤醒线程，当这个线程成功获取到锁后,于是就可以继续执⾏。

互斥锁加锁失败会存在一定的性能损耗(会有2次的线程上下文切换)
    1.当线程加锁失败时，内核会把线程的状态从「运⾏」状态设置为「睡眠」状态，然后把CPU 切换给其他线程运⾏。
    2.接着，当锁被释放时，之前「睡眠」状态的线程会变为「就绪」状态，然后内核会在合适的时间，把 CPU 切换给该线程运⾏。

线程的上下⽂切换的是什么？
   当两个线程是属于同⼀个进程，因为虚拟内存是共享的，所以在切换时,虚拟内存这些资源就保持不动,只需要切换线程的私有数据、寄存器等不共享的数据。
(上下切换的耗时有⼤佬统计过，⼤概在⼏⼗纳秒到⼏微秒之间，如果你锁住的代码执⾏时间⽐较短，那可能上下⽂切换的时间都⽐你锁住的代码执⾏时间还要⻓。)
所以，如果你能确定被锁住的代码执⾏时间很短，就不应该⽤互斥锁，⽽应该选⽤⾃旋锁，否则使⽤互斥锁。


自旋锁
   ⾃旋锁是通过 CPU 提供的 CAS 函数（Compare And Swap），在「⽤户态」完成加锁和解锁操作，
不会主动产⽣线程上下⽂切换，所以相⽐互斥锁来说，会快⼀些，开销也⼩⼀些。
  ⼀般加锁的过程，包含两个步骤：
    1.第⼀步，查看锁的状态，如果锁是空闲的，则执⾏第⼆步
    2.第⼆步，将锁设置为当前线程持有；
CAS 函数就把这两个步骤合并成⼀条硬件级指令，形成原⼦指令，这样就保证了这两个步骤是不可分割的，要么⼀次性执⾏完两个步骤，要么两个步骤都不执⾏。

  使⽤⾃旋锁的时候，当发⽣多线程竞争锁的情况，加锁失败的线程会「忙等待」，直到它拿到锁,这⾥的「忙等待」可以⽤ while 循环等待实现，不过最好是使⽤ CPU 提供的
PAUSE 指令来实现「忙等待」，因为可以减少循环等待时的耗电量。

  ⾃旋锁是最⽐较简单的⼀种锁，⼀直⾃旋，利⽤ CPU 周期，直到锁可⽤。需要注意，在单核 CPU 上，需要抢占式的调度器（即不断通过时钟中断⼀个线程，运⾏其他线程）.
否则，⾃旋锁在单 CPU 上⽆法使⽤，因为⼀个⾃旋的线程永远不会放弃 CPU。
  ⾃旋锁开销少，在多核系统下⼀般不会主动产⽣线程切换，适合异步、协程等在⽤户态切换请求的编程⽅式，但如果被锁住的代码执⾏时间过⻓，
⾃旋的线程会⻓时间占⽤ CPU 资源，所以⾃旋的时间和被锁住的代码执⾏的时间是成「正⽐」的关系，我们需要清楚的知道这⼀点。

  ⾃旋锁与互斥锁使⽤层⾯⽐较相似，但实现层⾯上完全不同：当加锁失败时，互斥锁⽤「线程切换」来应对，⾃旋锁则⽤「忙等待」来应对。
它俩是锁的最基本处理⽅式，更⾼级的锁都会选择其中⼀个来实现，⽐如读写锁既可以选择
互斥锁实现，也可以基于⾃旋锁实现。

  读写锁：读和写还有优先级区分？
  读写锁从字⾯意思我们也可以知道，它由「读锁」和「写锁」两部分构成,如果只读取共享
资源⽤「读锁」加锁，如果要修改共享资源则⽤「写锁」加锁。所以，读写锁适⽤于能明确区分读操作和写操作的场景

读写锁的⼯作原理是：
  1.当「写锁」没有被线程持有时，多个线程能够并发地持有读锁，这⼤⼤提⾼了共享资源的访问效率，因为「读锁」是⽤于读取共享资源的场景，
所以多个线程同时持有读锁也不会破坏共享资源的数据。
  2.但是，⼀旦「写锁」被线程持有后，读线程的获取读锁的操作会被阻塞，⽽且其他写线程的获取写锁的操作也会被阻塞。

所以说，写锁是独占锁，因为任何时刻只能有⼀个线程持有写锁，类似互斥锁和⾃旋锁，⽽读锁是共享锁，因为读锁可以被多个线程同时持有。
知道了读写锁的⼯作原理后，我们可以发现，读写锁在读多写少的场景，能发挥出优势。另外，根据实现的不同，读写锁可以分为「读优先锁」和「写优先锁」。

读写锁可以分为「读优先锁」和「写优先锁」
   读优先锁：读锁能被更多的线程持有，以便提⾼读线程的并发性，它的⼯作⽅式是：当读线程 A 先持有了读锁，写线程 B 在获取写锁的时候，会被阻塞，并且在阻塞过程
中，后续来的读线程 C 仍然可以成功获取读锁，最后直到读线程 A 和 C 释放读锁后，写线程B 才可以成功获取写锁。
   写优先锁：写优先锁是优先服务写线程，其⼯作⽅式是：当读线程 A 先持有了读锁，写线程 B 在获取写锁的时候，会被阻塞，并且在阻塞过程中，
后续来的读线程 C 获取读锁时会失败，于是读线程 C 将被阻塞在获取读锁的操作，这样只要读线程 A 释放读锁后，写线程 B 就可以成功获
取读锁。
   读优先锁对于读线程并发性更好，但也不是没有问题。我们试想⼀下，如果⼀直有读线程获取读锁，那么写线程将永远获取不到写锁，这就造成了写线程「饥饿」的现象


锁总结：
    开发过程中，最常⻅的就是互斥锁的了，互斥锁加锁失败时，会⽤「线程切换」来应对，当
加锁失败的线程再次加锁成功后的这⼀过程，会有两次线程上下⽂切换的成本，性能损耗⽐
较⼤.
   如果我们明确知道被锁住的代码的执⾏时间很短，那我们应该选择开销⽐较⼩的⾃旋锁，因
为⾃旋锁加锁失败时，并不会主动产⽣线程切换，⽽是⼀直忙等待，直到获取到锁，那么如
果被锁住的代码执⾏时间很短，那这个忙等待的时间相对应也很短。

   如果能区分读操作和写操作的场景，那读写锁就更合适了，它允许多个读线程可以同时持有
读锁，提⾼了读的并发性。根据偏袒读⽅还是写⽅，可以分为读优先锁和写优先锁，读优先
锁并发性很强，但是写线程会被饿死，⽽写优先锁会优先服务写线程，读线程也可能会被饿
死，那为了避免饥饿的问题，于是就有了公平读写锁，它是⽤队列把请求锁的线程排队，并
保证先⼊先出的原则来对线程加锁，这样便保证了某种线程不会被饿死，通⽤性也更好点
  另外，互斥锁、⾃旋锁、读写锁都属于悲观锁，悲观锁认为并发访问共享资源时，冲突概率
可能⾮常⾼，所以在访问共享资源前，都需要先加锁。



进程调度算法
   进程三大调度算法分别是，进程调度、页面置换、磁盘调度算法
什么情况会发生cpu调度？
   1.当进程从运⾏状态转到等待状态；
   2.当进程从运⾏状态转到就绪状态；
   3.当进程从等待状态转到就绪状态；
   4.当进程从运⾏状态转到终⽌状态；

其中发⽣在1和4两种情况下的调度称为「⾮抢占式调度」，2 和 3 两种情况下发⽣的调度称为「抢占式调度」。
    ⽽抢占式调度，顾名思义就是进程正在运⾏的时，可以被打断，使其把 CPU 让给其他进程。
那抢占的原则⼀般有三种，分别是时间⽚原则、优先权原则、短作业优先原则。
    假设有⼀个进程是处于等待状态的， 但是它的优先级⽐较⾼。如果该进程等待的事件发⽣了，它就会转到就绪状态，⼀旦它转到就绪状态
如果我们的调度算法是以优先级来进⾏调度的，那么它就会⽴⻢抢占正在运⾏的进程，所以这个时候就会发⽣ CPU调度。
    那第2种状态通常是时间⽚到的情况，因为时间⽚到了就会发⽣中断，于是就会抢占正在运⾏的进程，从⽽占⽤ CPU。

调度算法影响的是等待时间（进程在就绪队列中等待调度的时间总和），⽽不能影响进程真在使⽤ CPU 的时间和 I/O 时间。

常见进程调度算法
    1.先来先服务调度算法
    2.最短作业优先调度算法
    3.⾼响应⽐优先调度算法
    4.时间⽚轮转调度算法
    5.最⾼优先级调度算法
    6.多级反馈队列调度算法

先来先服务调度算法：
   每次从就绪队列选择最先进⼊队列的进程，然后⼀直运⾏，直到进程退出或被阻塞，才会继续从队列中选择第⼀个进程接着运⾏。

最短作业优先调度算法：
   最短作业优先（Shortest Job First, SJF）调度算法同样也是顾名思义，它会优先选择运⾏时间最短的进程来运⾏，这有助于提⾼系统的吞吐量。
⾼响应⽐优先调度算法：

   前⾯的「先来先服务调度算法」和「最短作业优先调度算法」都没有很好的权衡短作业和⻓作业，每次进⾏进程调度时，先计算「响应⽐优先级」，然后把「响应⽐优先级」最⾼的进程投⼊
运⾏，「响应⽐优先级」的计算公式： 优先权 = (等待时间+要求服务的时间)/要求的服务时间
   1.如果两个进程的「等待时间」相同时，「要求的服务时间」越短，「响应⽐」就越⾼，这样短作业的进程容易被选中运⾏；
   2.如果两个进程「要求的服务时间」相同时，「等待时间」越⻓，「响应⽐」就越⾼，这就兼顾到了⻓作业进程，因为进程的响应⽐可以随时间等待的增加⽽提⾼，当其等待时间⾜
     够⻓时，其响应⽐便可以升到很⾼，从⽽获得运⾏的机会；

 时间⽚轮转调度算法：
   最古⽼、最简单、最公平且使⽤最⼴的算法就是时间⽚轮转（Round Robin, RR）调度算法。
每个进程被分配⼀个时间段，称为时间⽚（Quantum），即允许该进程在该时间段中运⾏。
   1.如果时间⽚⽤完，进程还在运⾏，那么将会把此进程从 CPU 释放出来，并把 CPU 分配
另外⼀个进程；
   2.如果该进程在时间⽚结束前阻塞或结束，则 CPU ⽴即进⾏切换；
 另外，时间⽚的⻓度就是⼀个很关键的点：
   1.如果时间⽚设得太短会导致过多的进程上下⽂切换，降低了 CPU 效率
   2.如果设得太⻓⼜可能引起对短作业进程的响应时间变⻓。将通常时间⽚设为 20ms~50ms 通常是⼀个⽐较合理的折中值。

最⾼优先级调度算法
  但是，对于多⽤户计算机系统就有不同的看法了，它们希望调度是有优先级的，即希望调度
程序能从就绪队列中选择最⾼优先级的进程进⾏运⾏，这称为最⾼优先级（Highest PriorityFirst，HPF）调度算法。

进程的优先级可以分为，静态优先级或动态优先级：
  1.静态优先级：创建进程时候，就已经确定了优先级了，然后整个运⾏时间优先级都不会变化；
  2.动态优先级：根据进程的动态变化调整优先级，⽐如如果进程运⾏时间增加，则降低其优先级，如果进程等待时间（就绪队列的等待时间）增加，则升⾼其优先级，也就是随着时
    间的推移增加等待进程的优先级

该算法也有两种处理优先级⾼的⽅法，⾮抢占式和抢占式：
  1.⾮抢占式：当就绪队列中出现优先级⾼的进程，运⾏完当前进程，再选择优先级⾼的进程。
  2.抢占式：当就绪队列中出现优先级⾼的进程，当前进程挂起，调度优先级⾼的进程运⾏。
 但是依然有缺点，可能会导致低优先级的进程永远不会运⾏。

多级反馈队列调度算法？
  多级反馈队列（Multilevel Feedback Queue）调度算法是「时间⽚轮转算法」和「最⾼优先级算法」的综合和发展。
   1.「多级」表示有多个队列，每个队列优先级从⾼到低，同时优先级越⾼时间⽚越短。
   2.「反馈」表示如果有新的进程加⼊优先级⾼的队列时，⽴刻停⽌当前正在运⾏的进程，转⽽去运⾏优先级⾼的队列；

来看看，它是如何⼯作的：
   1.设置了多个队列，赋予每个队列不同的优先级，每个队列优先级从⾼到低，同时优先级越⾼时间⽚越短;
   2.新的进程会被放⼊到第⼀级队列的末尾，按先来先服务的原则排队等待被调度，如果在第⼀级队列规定的时间⽚没运⾏完成，则将其转⼊到第⼆级队列的末尾，以此类推，直⾄完成；
   3.当较⾼优先级的队列为空，才调度较低优先级的队列中的进程运⾏。如果进程运⾏时，有新进程进⼊较⾼优先级的队列，则停⽌当前运⾏的进程并将其移⼊到原队列末尾，接着让较⾼优先级的进程运⾏；

   可以发现，对于短作业可能可以在第⼀级队列很快被处理完。对于⻓作业，如果在第⼀级队
列处理不完，可以移⼊下次队列等待被执⾏，虽然等待的时间变⻓了，但是运⾏时间也会更
⻓了，所以该算法很好的兼顾了⻓短作业，同时有较好的响应时间。


内存⻚⾯置换算法:
   缺⻚异常（缺⻚中断）处理流程：
   1.在 CPU ⾥访问⼀条 Load M 指令，然后 CPU 会去找 M 所对应的⻚表项。
   2.如果该⻚表项的状态位是「有效的」，那 CPU 就可以直接去访问物理内存了，如果状态位是「⽆效的」，则 CPU 则会发送缺⻚中断请求。
   3.操作系统收到了缺⻚中断，则会执⾏缺⻚中断处理函数，先会查找该⻚⾯在磁盘中的⻚⾯的位置。
   4.找到磁盘中对应的⻚⾯后，需要把该⻚⾯换⼊到物理内存中，但是在换⼊前，需要在物理内存中找空闲⻚，如果找到空闲⻚，就把⻚⾯换⼊到物理内存中。
   5.⻚⾯从磁盘换⼊到物理内存完成后，则把⻚表项中的状态位修改为「有效的」。
   6.最后，CPU 重新执⾏导致缺⻚异常的指令。

上面第4步能在物理内存找到空闲页的情况，那如果找不到呢？
   找不到空闲⻚的话，就说明此时内存已满了，这时候，就需要「⻚⾯置换算法」选择⼀个物
理⻚，如果该物理⻚有被修改过（脏⻚），则把它换出到磁盘，然后把该被置换出去的⻚表
项的状态改成「⽆效的」，最后把正在访问的⻚⾯装⼊到这个物理⻚中

⻚表项:
   1.状态位：⽤于表示该⻚是否有效，也就是说是否在物理内存中，供程序访问时参考。
   2.访问字段：⽤于记录该⻚在⼀段时间被访问的次数，供⻚⾯置换算法选择出⻚⾯时参考。
   3.修改位：表示该⻚在调⼊内存后是否有被修改过，由于内存中的每⼀⻚都在磁盘上保留⼀
     份副本，因此，如果没有修改，在置换该⻚时就不需要将该⻚写回到磁盘上，以减少系统
     的开销；如果已经被修改，则将该⻚重写到磁盘上，以保证磁盘中所保留的始终是最新的
     副本。
   4.硬盘地址：⽤于指出该⻚在硬盘上的地址，通常是物理块号，供调⼊该⻚时使⽤


常⻅的⻚⾯置换算法有如下⼏种：
   1.最佳⻚⾯置换算法（OPT）
   2.先进先出置换算法（FIFO）
   3.最近最久未使⽤的置换算法（LRU）
   4.时钟⻚⾯置换算法（Lock）
   5.最不常⽤置换算法（LFU）

最佳⻚⾯置换算法:
   最佳⻚⾯置换算法基本思路是，置换在「未来」最⻓时间不访问的⻚⾯。
(所以，该算法实现需要计算内存中每个逻辑⻚⾯的「下⼀次」访问时间，然后⽐较，选择未来最⻓时间不访问的⻚⾯。)

先进先出置换算法:
   既然我们⽆法预知⻚⾯在下⼀次访问前所需的等待时间，那我们可以选择在内存驻留时间很
⻓的⻚⾯进⾏中置换，这个就是「先进先出置换」算法的思想。

最近最久未使⽤的置换算法:
   最近最久未使⽤（LRU）的置换算法的基本思路是，发⽣缺⻚时，选择最⻓时间没有被访问
的⻚⾯进⾏置换，也就是说，该算法假设已经很久没有使⽤的⻚⾯很有可能在未来较⻓的⼀
段时间内仍然不会被使⽤。

  这种算法近似最优置换算法，最优置换算法是通过「未来」的使⽤情况来推测要淘汰的⻚
⾯，⽽ LRU 则是通过「历史」的使⽤情况来推测要淘汰的⻚⾯。

时钟⻚⾯置换算法
   那有没有⼀种即能优化置换的次数，也能⽅便实现的算法呢？
时钟⻚⾯置换算法就可以两者兼得，它跟 LRU 近似，⼜是对 FIFO 的⼀种改进。
   该算法的思路是，把所有的⻚⾯都保存在⼀个类似钟⾯的「环形链表」中，⼀个表针指向最
⽼的⻚⾯。
  当发⽣缺⻚中断时，算法⾸先检查表针指向的⻚⾯
   1.如果它的访问位位是 0 就淘汰该⻚⾯，并把新的⻚⾯插⼊这个位置，然后把表针前移⼀个位置；
   2.如果访问位是 1 就清除访问位，并把表针前移⼀个位置，重复这个过程直到找到了⼀个访问位为 0 的⻚⾯为⽌；

最不常⽤算法
  当发⽣缺⻚中断时，选择「访问次数」最少的那个⻚⾯，并将其淘汰.
它的实现⽅式是，对每个⻚⾯设置⼀个「访问计数器」，每当⼀个⻚⾯被访问时，该⻚⾯的访问计数器就累加 1。在发⽣缺⻚中断时，淘汰计数器值最⼩的那个⻚⾯。
( 但还有个问题，LFU 算法只考虑了频率问题，没考虑时间的问题，⽐如有些⻚⾯在过去时间⾥访问的频率很⾼，但是现在已经没有访问了，
  ⽽当前频繁访问的⻚⾯由于没有这些⻚⾯访问的次数⾼，在发⽣缺⻚中断时，就会可能会误伤当前刚开始频繁访问，但访问次数还不⾼的⻚⾯
  那这个问题的解决的办法还是有的，可以定期减少访问的次数，⽐如当发⽣时间中断时，把过去时间访问的⻚⾯的访问次数除以 2，也就说，随着时间的流失，以前
  的⾼访问次数的⻚⾯会慢慢减少，相当于加⼤了被置换的概率。)


常见磁盘调度算法有哪些？
  1.先来先服务算法
    先来先服务，顾名思义，先到来的请求，先被服务。(请求访问的磁道可能会很分散，那先来先服务算法在性能上就会显得很差，因为寻道时间过⻓。)

  2.最短寻道时间优先算法
    最短寻道时间优先（Shortest Seek First，SSF）算法的⼯作⽅式是，优先选择从当前磁头位置所需寻道时间最短的请求,还是以这个序列为例⼦：
98，183，37，122，14，124，65，67  那么，那么根据距离磁头（ 53 位置）最近的请求的算法，具体的请求则会是下列从左到右的顺序：
65，67，37，14，98，122，124，183 ,但但这个算法可能存在某些请求的饥饿，如果后续来的请求都是⼩于 183磁道的，那么 183 磁道可能永远不会被响应，于是就产⽣了饥饿现象。

  3.扫描算法算法
    最短寻道时间优先算法会产⽣饥饿的原因在于：磁头有可能再⼀个⼩区域内来回得移动，为了防⽌这个问题，可以规定：磁头在⼀个⽅向上移动，访问所有未完成的请求，
直到磁头到达该⽅向上的最后的磁道，才调换⽅向，这就是扫描（Scan）算法，⽐如电梯保持按⼀个⽅向移动，直到在那个⽅向上没有请求为⽌，然后改变⽅向。

  4.循环扫描算法
    循环扫描（Circular Scan, CSCAN ）规定：只有磁头朝某个特定⽅向移动时，才处理磁道访
问请求，⽽返回时直接快速移动⾄最靠边缘的磁道，也就是复位磁头，这个过程是很快的，
并且返回中途不处理任何请求，该算法的特点，就是磁道只响应⼀个⽅向上的请求

  5.LOOK 与 C-LOOK 算法
    那针对 SCAN 算法的优化则叫 LOOK 算法，它的⼯作⽅式，磁头在每个⽅向上仅仅移动到最
远的请求位置，然后⽴即反向移动，⽽不需要移动到磁盘的最始端或最末端，反向移动的途
中会响应请求
   ⽽针 C-SCAN 算法的优化则叫 C-LOOK，它的⼯作⽅式，磁头在每个⽅向上仅仅移动到最远
的请求位置，然后⽴即反向移动，⽽不需要移动到磁盘的最始端或最末端，反向移动的途中
不会响应请求。


文件系统

  ⽂件系统的基本数据单位是⽂件，它的⽬的是对磁盘上的⽂件进⾏组织管理，那组织的⽅式
不同，就会形成不同的⽂件系统。Linux 最经典的⼀句话是：「⼀切皆⽂件」，不仅普通的⽂件和⽬录，就连块设备、管道、socket 等，也都是统⼀交给⽂件系统管理的.
Linux ⽂件系统会为每个⽂件分配两个数据结构：索引节点（index node）和⽬录项（directory entry），它们主要⽤来记录⽂件的元信息和⽬录层次结构。
  1.索引节点，也就是 inode，⽤来记录⽂件的元信息，⽐如 inode 编号、⽂件⼤⼩、访问权限、创建时间、修改时间、数据在磁盘的位置等等。索引节点是⽂件的唯⼀标识，它们之
    间⼀⼀对应，也同样都会被存储在硬盘中，所以索引节点同样占⽤磁盘空间。
  2.⽬录项，也就是 dentry，⽤来记录⽂件的名字、索引节点指针以及与其他⽬录项的层级关联关系。多个⽬录项关联起来，就会形成⽬录结构，但它与索引节点不同的是，⽬录项
    是由内核维护的⼀个数据结构，不存放于磁盘，⽽是缓存在内存。

索引节点是存储在硬盘上的数据，那么为了加速⽂件的访问，通常会把索引节点加载到内存中。
另外，磁盘进⾏格式化的时候，会被分成三个存储区域，分别是超级块、索引节点区和数据块区。
   1.超级块，⽤来存储⽂件系统的详细信息，⽐如块个数、块⼤⼩、空闲块等等。
   2.索引节点区，⽤来存储索引节点
   3.数据块区，⽤来存储⽂件或⽬录数据；

   我们不可能把超级块和索引节点区全部加载到内存，这样内存肯定撑不住，所以只有当需要
使⽤的时候，才将其加载进内存，它们加载进内存的时机是不同的：
   1.超级块：当⽂件系统挂载时进⼊内存；
   2.索引节点区：当⽂件被访问时进⼊内存；

虚拟⽂件系统
     ⽂件系统的种类众多，⽽操作系统希望对⽤户提供⼀个统⼀的接⼝，于是在⽤户层与⽂件系层引⼊了中间层，这个中间层就称为虚拟⽂件系统

Linux ⽀持的⽂件系统也不少，根据存储位置的不同，可以把⽂件系统分为三类：
    1.磁盘的⽂件系统，它是直接把数据存储在磁盘中，⽐如 Ext 2/3/4、XFS 等都是这类⽂件系统。
    2.内存的⽂件系统，这类⽂件系统的数据不是存储在硬盘的，⽽是占⽤内存空间，我们经常⽤到的 /proc 和 /sys ⽂件系统都属于这⼀类，读写这类⽂件，实际上是读写内核中相关的数据
    3.⽹络的⽂件系统，⽤来访问其他计算机主机数据的⽂件系统，⽐如 NFS、SMB 等等

⽂件系统⾸先要先挂载到某个⽬录才可以正常使⽤，⽐如 Linux 系统在启动时，会把⽂件系统挂载到根⽬录

⽂件的使⽤
  1.⾸先⽤ open 系统调⽤打开⽂件， open 的参数中包含⽂件的路径名和⽂件名。
  2.使⽤ write 写数据，其中 write 使⽤ open 所返回的⽂件描述符，并不使⽤⽂件名作为参数。
  3.使⽤完⽂件后，要⽤ close 系统调⽤关闭⽂件，避免资源的泄露。

我们打开了⼀个⽂件后，操作系统会跟踪进程打开的所有⽂件，所谓的跟踪呢，就是操作系统为每个进程维护⼀个打开⽂件表，⽂件表⾥的每⼀项代表「⽂件描述符」，所以说⽂件描
述符是打开⽂件的标识

操作系统在打开⽂件表中维护着打开⽂件的状态和信息：
  1.⽂件指针：系统跟踪上次读写位置作为当前⽂件位置指针，这种指针对打开⽂件的某个进程来说是唯⼀的；
  2.⽂件打开计数器：⽂件关闭时，操作系统必须重⽤其打开⽂件表条⽬，否则表内空间不够⽤。因为多个进程可能打开同⼀个⽂件，所以系统在删除打开⽂件条⽬之前，必须等待最后⼀个进程关闭⽂件，
该计数器跟踪打开和关闭的数量，当该计数为 0 时，系统关闭⽂件，删除该条⽬；
  3.⽂件磁盘位置：绝⼤多数⽂件操作都要求系统修改⽂件数据，该信息保存在内存中，以免每个操作都从磁盘中读取；
  4.访问权限：每个进程打开⽂件都需要有⼀个访问模式（创建、只读、读写、添加等），该信息保存在进程的打开⽂件表中，以便操作系统能允许或拒绝之后的 I/O 请求；


我们来分别看⼀下，读⽂件和写⽂件的过程：
  1.当⽤户进程从⽂件读取 1 个字节⼤⼩的数据时，⽂件系统则需要获取字节所在的数据块，再返回数据块对应的⽤户进程所需的数据部分
  2.当⽤户进程把 1 个字节⼤⼩的数据写进⽂件时，⽂件系统则找到需要写⼊数据的数据块的位置，然后修改数据块中对应的部分，最后再把数据块写回磁盘。

所以说，⽂件系统的基本操作单位是数据块。


⽂件的存储
  ⽂件的数据是要存储在硬盘上⾯的，数据在磁盘上的存放⽅式，就像程序在内存中存放的⽅式那样，有以下两种：
    1.连续空间存放⽅式
    2.⾮连续空间存放⽅式

其中，⾮连续空间存放⽅式⼜可以分为「链表⽅式」和「索引⽅式」。
  连续空间存放⽅式:
连续空间存放⽅式顾名思义，⽂件存放在磁盘「连续的」物理空间中。这种模式下，⽂件的数据都是紧密相连，读写效率很⾼，因为⼀次磁盘寻道就可以读出整个⽂件.
使⽤连续存放的⽅式有⼀个前提，必须先知道⼀个⽂件的⼤⼩，这样⽂件系统才会根据⽂件的⼤⼩在磁盘上找到⼀块连续的空间分配给⽂件。
  ⽂件头⾥需要指定「起始块的位置」和「⻓度」,有了这两个信息就可以很好的表示⽂件存放⽅式是⼀块连续的磁盘空间
(连续空间存放的⽅式虽然读写效率⾼，但是有「磁盘空间碎⽚」和「⽂件⻓度不易扩展」的缺陷。)
   如果⽂件 B 被删除，磁盘上就留下⼀块空缺，这时，如果新来的⽂件⼩于其中的⼀
个空缺，我们就可以将其放在相应空缺⾥。但如果该⽂件的⼤⼩⼤于所有的空缺，但却⼩于
空缺⼤⼩之和，则虽然磁盘上有⾜够的空缺，但该⽂件还是不能存放。当然了，我们可以通
过将现有⽂件进⾏挪动来腾出空间以容纳新的⽂件，但是这个在磁盘挪动⽂件是⾮常耗时，
所以这种⽅式不太现实。

⾮连续空间存放⽅式(⾮连续空间存放⽅式分为「链表⽅式」和「索引⽅式」。)
    链表的⽅式存放是离散的，不⽤连续的，于是就可以消除磁盘碎⽚，可⼤⼤提⾼磁盘空间的
利⽤率，同时⽂件的⻓度可以动态扩展。根据实现的⽅式的不同，链表可分为「隐式链表」
和「显式链接」两种形式。
   1.「隐式链表」的⽅式存放的话，实现的⽅式是⽂件头要包含「第⼀块」和「最后⼀块」的位置，并且每个数据块⾥⾯留出⼀个指针空间，⽤来存放下⼀个数据块的位置，这样
⼀个数据块连着⼀个数据块，从链头开是就可以顺着指针找到所有的数据块，所以存放的⽅式可以是不连续的。
   隐式链表的存放⽅式的缺点:在于⽆法直接访问数据块，只能通过指针顺序访问⽂件，以及数据块指针消耗了⼀定的存储空间。隐式链接分配的稳定性较差，系统在运⾏过程中由于软件
或者硬件错误导致链表中的指针丢失或损坏，会导致⽂件数据的丢失。
   2.显式链接:它指把⽤于链接⽂件各数据块的指针，显式地存放在内存的⼀张链接表中，该表在整个磁盘仅设置⼀张，每个表项中存放链接指针，指向下
⼀个数据块号。(但也正是整个表都存放在内存中的关系，它的主要的缺点是不适⽤于⼤磁盘)


早期Unix ⽂件系统是：
  1.如果存放⽂件所需的数据块⼩于 10 块，则采⽤直接查找的⽅式
  2.如果存放⽂件所需的数据块超过 10 块，则采⽤⼀级间接索引⽅式；
  3.如果前⾯两种⽅式都不够存放⼤⽂件，则采⽤⼆级间接索引⽅式；
  4.如果⼆级间接索引也不够存放⼤⽂件，这采⽤三级间接索引⽅式；

   在 Linux ⽂件系统就采⽤了位图的⽅式来管理空闲空间,不仅⽤于数据空闲块的管理，还⽤
于 inode 空闲块的管理，因为 inode 也是存储在磁盘的，⾃然也要有对其管理。


   ⽂件 I/O

常见的文件io有多个：
   1.缓冲与⾮缓冲 I/O
   2.直接与⾮直接 I/O
   3.阻塞与⾮阻塞 I/O VS 同步与异步 I/O

缓冲与⾮缓冲 I/O?
   ⽂件操作的标准库是可以实现数据的缓存，那么根据「是否利⽤标准库缓冲」，可以把⽂件I/O 分为缓冲 I/O 和⾮缓冲 I/O
   1.缓冲 I/O，利⽤的是标准库的缓存实现⽂件的加速访问，⽽标准库再通过系统调⽤访问⽂件。
   2.⾮缓冲 I/O，直接通过系统调⽤访问⽂件，不经过标准库缓存.
   ⽐⽅说，很多程序遇到换⾏时才真正输出，⽽换⾏前的内容，其实就是被标准库暂时缓存了起来，这样做的⽬的是，减少系统调⽤的次数，毕竟系统调⽤是有 CPU 上下⽂切换的开销的。

直接与⾮直接 I/O
  我们都知道磁盘 I/O 是⾮常慢的，所以 Linux 内核为了减少磁盘 I/O 次数，在系统调⽤后，会
  把⽤户数据拷⻉到内核中缓存起来，这个内核缓存空间也就是「⻚缓存」，只有当缓存满⾜
  某些条件的时候，才发起磁盘 I/O 的请求

  1.直接 I/O，不会发⽣内核缓存和⽤户程序之间数据复制，⽽是直接经过⽂件系统访问磁盘。
  2.⾮直接 I/O，读操作时，数据从内核缓存中拷⻉给⽤户程序，写操作时，数据从⽤户程序拷⻉给内核缓存，再由内核决定什么时候写⼊数据到磁盘

⼏种场景会触发内核缓存的数据写⼊磁盘：
  1.在调⽤ write 的最后，当发现内核缓存的数据太多的时候，内核会把数据写到磁盘上；
  2.⽤户主动调⽤ sync ，内核缓存会刷到磁盘上；
  3.当内存⼗分紧张，⽆法再分配⻚⾯时，也会把内核缓存的数据刷到磁盘上；
  4.内核缓存的数据的缓存时间超过某个时间时，也会把数据刷到磁盘上；

阻塞与⾮阻塞 I/O VS 同步与异步 I/O
   阻塞 I/O:当⽤户程序执⾏ read ，线程会被阻塞，⼀直等到内核数据准备好，并把数据从内核缓冲区拷⻉到应⽤程序的缓冲区中，当拷⻉过程完成， read 才会返回
   阻塞等待的是「内核数据准备好」和「数据从内核态拷⻉到⽤户态」这两个过程

   ⾮阻塞 I/O:⾮阻塞的 read 请求在数据未准备好的情况下⽴即返回，可以继续往下执⾏，此时应⽤程序不断轮询内核，直到数据准备好，内核将数据拷⻉到
 应⽤程序缓冲区， read 调⽤才可以获取到结果

   I/O 多路复⽤技术就出来了,如 select、poll，它是通过
I/O 事件分发，当内核数据准备好时，再以事件通知应⽤程序进⾏操作。

  实际上，⽆论是阻塞 I/O、⾮阻塞 I/O，还是基于⾮阻塞 I/O 的多路复⽤都是同步调⽤。
因为它们在 read 调⽤时，内核将数据从内核空间拷⻉到应⽤程序空间，过程都是需要等待的，也就是说这个过程是同步的，如果内核实现的拷⻉效率不⾼，read 调⽤就会在这个同步过程中
等待⽐较⻓的时间。
  ⽽真正的异步 I/O 是「内核数据准备好」和「数据从内核态拷⻉到⽤户态」这两个过程都不⽤等待。


linux网络模型
  为了使得多种设备能通过⽹络相互通信，和为了解决各种不同设备在⽹络互联中的兼容性问
题，国际标标准化组织制定了开放式系统互联通信参考模型，也就是 OSI ⽹络模型，该模型主要有 7 层，分别是应⽤层、表示层、
会话层、传输层、⽹络层、数据链路层以及物理层。
每⼀层负责的职能都不同，如下：
   1.应⽤层，负责给应⽤程序提供统⼀的接⼝；
   2.表示层，负责把数据转换成兼容另⼀个系统能识别的格式；
   3.会话层，负责建⽴、管理和终⽌表示层实体之间的通信会话；
   4.传输层，负责端到端的数据传输；
   5.⽹络层，负责数据的路由、转发、分⽚；
   6.数据链路层，负责数据的封帧和差错检测，以及 MAC 寻址；
   7.物理层，负责在物理⽹络中传输数据帧；

TCP/IP ⽹络模型共有 4 层，分别是应⽤层、传输层、⽹络层和⽹络接⼝层，每⼀层负责的职能如下：
   1.应⽤层，负责向⽤户提供⼀组应⽤程序，⽐如 HTTP、DNS、FTP 等;
   2.传输层，负责端到端的通信，⽐如 TCP、UDP 等；
   3.⽹络层，负责⽹络包的封装、分⽚、路由、转发，⽐如 IP、ICMP 等；
   4.⽹络接⼝层，负责⽹络包在物理⽹络中的传输，⽐如⽹络包的封帧、 MAC 寻址、差错检测，以及通过⽹卡传输⽹络帧等；

不过，我们常说的七层和四层负载均衡，是⽤ OSI ⽹络模型来描述的，七层对应的是应⽤层，四层对应的是传输层。

Linux 接收⽹络包的流程？
   ⽹卡是计算机⾥的⼀个硬件，专⻔负责接收和发送⽹络包，当⽹卡接收到⼀个⽹络包后，会通过 DMA 技术，将⽹络包放⼊到 Ring Buffer，这个是⼀个环形缓冲区。
那接收到⽹络包后，应该怎么告诉操作系统这个⽹络包已经到达了呢？最简单的⼀种⽅式就是触发中断，也就是每当⽹卡收到⼀个⽹络包，就触发⼀个中断告诉操作系统。
   ⽐如，当有⽹络包到达时，⽹卡发起硬件中断，于是会执⾏⽹卡硬件中断处理函数，中断处
理函数处理完需要「暂时屏蔽中断」，然后唤醒「软中断」来轮询处理数据，直到没有新数
据时才恢复中断，这样⼀次中断处理多个⽹络包，于是就可以降低⽹卡中断带来的性能开
销。那软中断是怎么处理⽹络包的呢？它会从 Ring Buffer 中拷⻉数据到内核 struct sk_buff 缓冲区中，从⽽可以作为⼀个⽹络包交给⽹络协议栈进⾏逐层处理。
   ⾸先，会先进⼊到⽹络接⼝层，在这⼀层会检查报⽂的合法性，如果不合法则丢弃，合法则
会找出该⽹络包的上层协议的类型，⽐如是 IPv4，还是 IPv6，接着再去掉帧头和帧尾，然后
交给⽹络层。
  到了⽹络层，则取出 IP 包，判断⽹络包下⼀步的⾛向，⽐如是交给上层处理还是转发出去。
当确认这个⽹络包要发送给本机后，就会从 IP 头⾥看看上⼀层协议的类型是 TCP 还是
UDP，接着去掉 IP 头，然后交给传输层。
  传输层取出 TCP 头或 UDP 头，根据四元组「源 IP、源端⼝、⽬的 IP、⽬的端⼝」 作为标
识，找出对应的 Socket，并把数据拷⻉到 Socket 的接收缓冲区。
  最后，应⽤层程序调⽤ Socket 接⼝，从内核的 Socket 接收缓冲区读取新到来的数据到应⽤
层。

Linux 发送⽹络包的流程？
   ⾸先，应⽤程序会调⽤ Socket 发送数据包的接⼝，由于这个是系统调⽤，所以会从⽤户态陷
⼊到内核态中的 Socket 层，Socket 层会将应⽤层数据拷⻉到 Socket 发送缓冲区中。
   接下来，⽹络协议栈从 Socket 发送缓冲区中取出数据包，并按照 TCP/IP 协议栈从上到下逐
层处理。
   如果使⽤的是 TCP 传输协议发送数据，那么会在传输层增加 TCP 包头，然后交给⽹络层，
⽹络层会给数据包增加 IP 包，然后通过查询路由表确认下⼀跳的 IP，并按照 MTU ⼤⼩进⾏分⽚。
   分⽚后的⽹络包，就会被送到⽹络接⼝层，在这⾥会通过 ARP 协议获得下⼀跳的 MAC 地
址，然后增加帧头和帧尾，放到发包队列中。
   这⼀些准备好后，会触发软中断告诉⽹卡驱动程序，这⾥有新的⽹络包需要发送，最后驱动
程序通过 DMA，从发包队列中读取⽹络包，将其放⼊到硬件⽹卡的队列中，随后物理⽹卡再
将它发送出去。

零拷⻉？
 什么是 DMA 技术？
 在进⾏ I/O 设备和内存的数据传输的时候，数据搬运的⼯
作全部交给 DMA 控制器，⽽ CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以
去处理别的事务。

DMA控制器进行数据传输流程：
 1.⽤户进程调⽤ read ⽅法，向操作系统发出 I/O 请求，请求读取数据到⾃⼰的内存缓冲区中，进程进⼊阻塞状态；
 2.操作系统收到请求后，进⼀步将 I/O 请求发送 DMA，然后让 CPU 执⾏其他任务；
 3.DMA 进⼀步将 I/O 请求发送给磁盘；
 4.磁盘收到 DMA 的 I/O 请求，把数据从磁盘读取到磁盘控制器的缓冲区中，当磁盘控制器的缓冲区被读满后，向 DMA 发起中断信号，告知⾃⼰缓冲区已满；
 5.DMA 收到磁盘的信号，将磁盘控制器缓冲区中的数据拷⻉到内核缓冲区中，此时不占⽤CPU，CPU 可以执⾏其他任务；
 7.当 DMA 读取了⾜够多的数据，就会发送中断信号给 CPU；
 8.CPU 收到 DMA 的信号，知道数据已经准备好，于是将数据从内核拷⻉到⽤户空间，系统调⽤返回；

   可以看到， 整个数据传输的过程，CPU 不再参与数据搬运的⼯作，⽽是全程由 DMA 完成，
但是 CPU 在这个过程中也是必不可少的，因为传输什么数据，从哪⾥传输到哪⾥，都需要
CPU 来告诉 DMA 控制器。早期 DMA 只存在在主板上，如今由于 I/O 设备越来越多，数据传输的需求也不尽相同，所以
每个 I/O 设备⾥⾯都有⾃⼰的 DMA 控制器

传统 I/O 的⼯作⽅式是，数据读取和写⼊是从⽤户空间到内核空间来回复制，⽽内核空间的
数据是通过操作系统层⾯的 I/O 接⼝从磁盘读取或写⼊。一般会需要两个系统调⽤：
  期间共发⽣了 4 次⽤户态与内核态的上下⽂切换，因为发⽣了两次系统调⽤，⼀次是
read() ，⼀次是 write() ，每次系统调⽤都得先从⽤户态切换到内核态，等内核完成任务
后，再从内核态切换回⽤户态。

其次，还发⽣了 4 次数据拷⻉，其中两次是 DMA 的拷⻉，另外两次则是通过 CPU 拷⻉的，下⾯说⼀下这个过程：
1.第⼀次拷⻉，把磁盘上的数据拷⻉到操作系统内核的缓冲区⾥，这个拷⻉的过程是通过DMA 搬运的。
2.第⼆次拷⻉，把内核缓冲区的数据拷⻉到⽤户的缓冲区⾥，于是我们应⽤程序就可以使⽤这部分数据了，这个拷⻉到过程是由 CPU 完成的。
3.第三次拷⻉，把刚才拷⻉到⽤户的缓冲区⾥的数据，再拷⻉到内核的 socket 的缓冲区⾥，这个过程依然还是由 CPU 搬运的。
4.第四次拷⻉，把内核的 socket 缓冲区⾥的数据，拷⻉到⽹卡的缓冲区⾥，这个过程⼜是由 DMA 搬运的。

如何实现零拷⻉？
   零拷⻉技术实现的⽅式通常有 2 种：
   1.mmap + write
   2.sendfile

   mmap() 系统调⽤函数会直接把内核缓冲区⾥的数据「映射」到⽤户空间，这样，操作系统
内核与⽤户空间就不需要再进⾏任何的数据拷⻉操作。具体过程如下：
   1.应⽤进程调⽤了 mmap() 后，DMA 会把磁盘的数据拷⻉到内核的缓冲区⾥。接着，应⽤进程跟操作系统内核「共享」这个缓冲区；
   2.应⽤进程再调⽤ write() ，操作系统直接将内核缓冲区的数据拷⻉到 socket 缓冲区中，这⼀切都发⽣在内核态，由 CPU 来搬运数据；
   3.最后，把内核的 socket 缓冲区⾥的数据，拷⻉到⽹卡的缓冲区⾥，这个过程是由 DMA搬运的。

但这还不是最理想的零拷⻉，因为仍然需要通过 CPU 把内核缓冲区的数据拷⻉到 socket 缓
冲区⾥，⽽且仍然需要 4 次上下⽂切换，因为系统调⽤还是 2 次。

sendfile?

    ⾸先，它可以替代前⾯的 read() 和 write() 这两个系统调⽤，这样就可以减少⼀次系统调⽤，也就减少了 2 次上下⽂切换的开销。其次，该系统调⽤，可以直接把内核缓冲区⾥的数据拷⻉到 socket 缓冲区⾥，不再拷⻉到⽤
户态，这样就只有 2 次上下⽂切换，和 3 次数据拷⻉.
     但是这还不是真正的零拷⻉技术，如果⽹卡⽀持 SG-DMA（The Scatter-Gather Direct
Memory Access）技术（和普通的 DMA 有所不同），我们可以进⼀步减少通过 CPU 把内核
缓冲区⾥的数据拷⻉到 socket 缓冲区的过程.于是，从 Linux 内核 2.4 版本开始起，对于⽀持⽹卡⽀持 SG-DMA 技术的情况下，
sendfile() 系统调⽤的过程发⽣了点变化，具体过程如下：
    1.第⼀步，通过 DMA 将磁盘上的数据拷⻉到内核缓冲区⾥；
    2.第⼆步，缓冲区描述符和数据⻓度传到 socket 缓冲区，这样⽹卡的 SG-DMA 控制器就
      可以直接将内核缓存中的数据拷⻉到⽹卡的缓冲区⾥，此过程不需要将数据从操作系统内
      核缓冲区拷⻉到 socket 缓冲区中，这样就减少了⼀次数据拷⻉；

这就是所谓的零拷⻉（Zero-copy）技术，因为我们没有在内存层⾯去拷⻉数据，也就是说全
程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进⾏传输的。零拷⻉技术的⽂件传输⽅式相⽐传统⽂件传输的⽅式，减少了 2 次上下⽂切换和数据拷⻉次
数，只需要 2 次上下⽂切换和数据拷⻉次数，就可以完成⽂件的传输，⽽且 2 次的数据拷⻉过程，都不需要通过 CPU，2 次都是由 DMA 来搬运。
所以，总体来看，零拷⻉技术可以把⽂件传输的性能提⾼⾄少⼀倍以上。


PageCache 有什么作⽤？
    其中第⼀步都是先需要先把磁盘⽂件数据拷⻉「内核缓冲区」⾥，这个「内核缓冲区」实际上是磁盘⾼速缓存（PageCache）。
所以，读磁盘数据的时候，优先在 PageCache 找，如果数据存在则可以直接返回；如果没有，则从磁盘中读取，然后缓存 PageCache 中。

所以，PageCache 的优点主要是两个：
   1.缓存最近被访问的数据；
   2.预读功能；


大文件传输
   我们先来看看最初的例⼦，当调⽤ read ⽅法读取⽂件时，进程实际上会阻塞在 read ⽅法调
⽤，因为要等待磁盘数据的返回的流程：
   1.当调⽤ read ⽅法时，会阻塞着，此时内核会向磁盘发起 I/O 请求，磁盘收到请求后，便会寻址，当磁盘数据准备好后，就会向内核发起 I/O 中断，告知内核磁盘数据已经准备好；
   2.内核收到 I/O 中断后，就将数据从磁盘控制器缓冲区拷⻉到 PageCache ⾥；
   3.最后，内核再把 PageCache 中的数据拷⻉到⽤户缓冲区，于是 read 调⽤就正常返回了。

对于阻塞的问题，可以⽤异步 I/O 来解决，它⼯作⽅式如下图：
   1.前半部分，内核向磁盘发起读请求，但是可以不等待数据就位就可以返回，于是进程此时可以处理其他任务；
   2.后半部分，当内核将磁盘中的数据拷⻉到进程缓冲区后，进程将接收到内核的通知，再去处理数据

IO相关定义(绕开 PageCache 的 I/O 叫直接 I/O，使⽤ PageCache 的 I/O 则叫缓存 I/O。通常，对于磁盘，异步 I/O 只⽀持直接 I/O。),于是，在⾼并发的场景下，
针对⼤⽂件的传输的⽅式，应该使⽤「异步 I/O + 直接 I/O」来替代零拷⻉技术

直接 I/O 应⽤场景常⻅的两种：
 1.应⽤程序已经实现了磁盘数据的缓存，那么可以不需要 PageCache 再次缓存，减少额外
   的性能损耗。在 MySQL 数据库中，可以通过参数设置开启直接 I/O，默认是不开启；
 2.传输⼤⽂件的时候，由于⼤⽂件难以命中 PageCache 缓存，⽽且会占满 PageCache 导致「热点」⽂件⽆法充分利⽤缓存，从⽽增⼤了性能开销，因此，这时应该使⽤直接I/O。


I/O 多路复⽤：select/poll/epoll?
   最基本的 Socket 模型,要想客户端和服务器能在⽹络中通信，那必须得使⽤ Socket 编程，它是进程间通信⾥⽐较特别的⽅式，特别之处在于它是可以跨主机间通信。
创建 Socket 的时候，可以指定⽹络层使⽤的是 IPv4 还是 IPv6，传输层使⽤的是 TCP 还是UDP。
   UDP 的 Socket 编程相对简单些，这⾥我们只介绍基于 TCP 的 Socket 编程。

服务端Socket编程过程是怎样的?
    服务端⾸先调⽤ socket() 函数，创建⽹络协议为 IPv4，以及传输协议为 TCP 的 Socket ，接着调⽤ bind() 函数，给这个 Socket 绑定⼀个 IP 地址和端⼝，绑定这两个的⽬的是什
么？
    1.绑定端⼝的⽬的：当内核收到 TCP 报⽂，通过 TCP 头⾥⾯的端⼝号，来找到我们的应⽤程序，然后把数据传递给我们.
    2.绑定 IP 地址的⽬的：⼀台机器是可以有多个⽹卡的，每个⽹卡都有对应的 IP 地址，当绑定⼀个⽹卡时，内核在收到该⽹卡上的包，才会发给我们；
    3.绑定完 IP 地址和端⼝后，就可以调⽤ listen() 函数进⾏监听，此时对应 TCP 状态图中的listen ，如果我们要判定服务器中⼀个⽹络程序有没有启动，可以通过 netstat 命令查看
    对应的端⼝号是否有被监听。
    4.服务端进⼊了监听状态后，通过调⽤ accept() 函数，来从内核获取客户端的连接，如果没
      有客户端连接，则会阻塞等待客户端连接的到来。

客户端是怎么发起连接的呢？
    客户端在创建好 Socket 后，调⽤ connect() 函数发起连
接，该函数的参数要指明服务端的 IP 地址和端⼝号，然后万众期待的 TCP 三次握⼿就开始了。
在 TCP 连接的过程中，服务器的内核实际上为每个 Socket 维护了两个队列：
    1.⼀个是还没完全建⽴连接的队列，称为 TCP 半连接队列，这个队列都是没有完成三次握⼿的连接，此时服务端处于 syn_rcvd 的状态；
    2.⼀个是⼀件建⽴连接的队列，称为 TCP 全连接队列，这个队列都是完成了三次握⼿的连接，此时服务端处于 established 状态；

当 TCP 全连接队列不为空后，服务端的 accept() 函数，就会从内核中的 TCP 全连接队列⾥拿出⼀个已经完成连接的 Socket 返回应⽤程序，后续数据传输都⽤这个 Socket。
注意，监听的 Socket 和真正⽤来传数据的 Socket 是两个:
    1.一个叫作监听 Socket；
    2.⼀个叫作已连接 Socket；

如何服务更多的⽤户？
    前⾯提到的 TCP Socket 调⽤流程是最简单、最基本的，它基本只能⼀对⼀通信，因为使⽤
的是同步阻塞的⽅式，当服务端在还没处理完⼀个客户端的⽹络 I/O 时，或者 读写操作发⽣
阻塞时，其他客户端是⽆法与服务端连接的。

在改进⽹络 I/O 模型前，我先来提⼀个问题，你知道服务器单机理论最⼤能连接多少个客户端？
    相信你知道 TCP 连接是由四元组唯⼀确认的，这个四元组就是：本机IP, 本机端⼝, 对端IP, 对端端⼝。
于是对于服务端 TCP 连接的四元组只有对端 IP 和端⼝是会变化的，所以最⼤ TCP 连接数 = 客户端 IP 数×客户端端⼝数。
对于 IPv4，客户端的 IP 数最多为 2 的 32 次⽅，客户端的端⼝数最多为 2 的 16 次⽅，也就是服务端单机最⼤ TCP 连接数约为 2 的 48 次⽅。


这个理论值相当“丰满”，但是服务器肯定承载不了那么⼤的连接数，主要会受两个⽅⾯的限制：
   1.⽂件描述符，Socket 实际上是⼀个⽂件，也就会对应⼀个⽂件描述符。在 Linux 下，单个进程打开的⽂件描述符数是有限制的，没有经过修改的值⼀般都是 1024，不过我们可以通过 ulimit 增⼤⽂件描述符的数⽬；
   2.系统内存，每个 TCP 连接在内核中都有对应的数据结构，意味着每个连接都是会占⽤⼀定内存的；


多进程模型
  基于最原始的阻塞⽹络 I/O， 如果服务器要⽀持多个客户端，其中⽐较传统的⽅式，就是使⽤多进程模型，也就是为每个客户端分配⼀个进程来处理请求。
服务器的主进程负责监听客户的连接，⼀旦与客户端连接完成，accept() 函数就会返回⼀个「已连接 Socket」，这时就通过 fork() 函数创建⼀个⼦进程，实际上就把⽗进程所有相关
的东⻄都复制⼀份，包括⽂件描述符、内存地址空间、程序计数器、执⾏的代码等。
   这种⽤多个进程来应付多个客户端的⽅式，在应对 100 个客户端还是可⾏的，但是当客户端
数量⾼达⼀万时，肯定扛不住的，因为每产⽣⼀个进程，必会占据⼀定的系统资源，⽽且进程间上下⽂切换的“包袱”是很重的，性能会⼤打折扣。进程的上下⽂切换不仅包含了虚拟内存、栈、全局变量等⽤户空间的资源，还包括了内核堆
栈、寄存器等内核空间的资源。


多线程模型
  既然进程间上下⽂切换的“包袱”很重，那我们就搞个⽐较轻量级的模型来应对多⽤户的请求
  —— 多线程模型。

线程是运⾏在进程中的⼀个“逻辑流”，单进程中可以运⾏多个线程，同进程⾥的线程可以共享进程的部分资源的，⽐如⽂件描述符列表、进程空间、代码、全局数据、堆、共享库等，因此同⼀个进程下的线程上下⽂切换的开销要⽐进程⼩得多。
当服务器与客户端 TCP 完成连接后，通过 pthread_create() 函数创建线程，然后将「已连接 Socket」的⽂件描述符传递给线程函数，接着在线程⾥和客户端进⾏通信，从⽽达到并发处理的⽬的。
如果每来⼀个连接就创建⼀个线程，线程运⾏完后，还得操作系统还得销毁线程，虽说线程切换的上写⽂开销不⼤，但是如果频繁创建和销毁线程，系统开销也是不⼩的。那么，我们可以使⽤线程池的⽅式来避免线程的频繁创建和销毁，所谓的线程池，就是提前
创建若⼲个线程，这样当由新连接建⽴时，将这个已连接的 Socket 放⼊到⼀个队列⾥，然后线程池⾥的线程负责从队列中取出已连接 Socket 进程处理。

  需要注意的是，这个队列是全局的，每个线程都会操作，为了避免多线程竞争，线程在操作这个队列前要加锁

I/O 多路复⽤:
   既然为每个请求分配⼀个进程/线程的⽅式不合适，那有没有可能只使⽤⼀个进程来维护多个Socket 呢？答案是有的，那就是 I/O 多路复⽤技术。
select/poll/epoll 内核提供给⽤户态的多路复⽤系统调⽤，进程可以通过⼀个系统调⽤函数从内核中获取多个事件。
   select/poll/epoll 是如何获取⽹络事件的呢？在获取事件时，先把所有连接（⽂件描述符）传给内核，再由内核返回产⽣了事件的连接，然后在⽤户态中再处理这些连接对应的请求即
可。

  select/poll:
    select 实现多路复⽤的⽅式是，将已连接的 Socket 都放到⼀个⽂件描述符集合，然后调⽤
select 函数将⽂件描述符集合拷⻉到内核⾥，让内核来检查是否有⽹络事件产⽣，检查的⽅
式很粗暴，就是通过遍历⽂件描述符集合的⽅式，当检查到有事件产⽣后，将此 Socket 标记
为可读或可写，接着再把整个⽂件描述符集合拷⻉回⽤户态⾥，然后⽤户态还需要再通过遍历的
⽅法找到可读或可写的Socket，然后再对其处理.
  所以，对于 select 这种⽅式，需要进⾏ 2 次「遍历」⽂件描述符集合，⼀次是在内核态⾥，
⼀个次是在⽤户态⾥ ，⽽且还会发⽣ 2 次「拷⻉」⽂件描述符集合，先从⽤户空间传⼊内核
空间，由内核修改后，再传出到⽤户空间中。
  select 使⽤固定⻓度的 BitsMap，表示⽂件描述符集合，⽽且所⽀持的⽂件描述符的个数是
有限制的，在 Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最⼤值为 1024 ，只能监
听 0~1023 的⽂件描述符。
  poll 不再⽤ BitsMap 来存储所关注的⽂件描述符，取⽽代之⽤动态数组，以链表形式来组
织，突破了 select 的⽂件描述符个数限制，当然还会受到系统⽂件描述符限制。
  但是 poll 和 select 并没有太⼤的本质区别，都是使⽤「线性结构」存储进程关注的 Socket
集合，因此都需要遍历⽂件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，⽽
且也需要在⽤户态与内核态之间拷⻉⽂件描述符集合，这种⽅式随着并发数上来，性能的损
耗会呈指数级增⻓。

  epoll
  1.第⼀点，epoll 在内核⾥使⽤红⿊树来跟踪进程所有待检测的⽂件描述字，把需要监控的
    socket 通过 epoll_ctl() 函数加⼊内核中的红⿊树⾥，红⿊树是个⾼效的数据结构，增删
    查⼀般时间复杂度是 O(logn) ，通过对这棵⿊红树进⾏操作，这样就不需要像 select/poll 每
    次操作时都传⼊整个 socket 集合，只需要传⼊⼀个待检测的 socket，减少了内核和⽤户空间
    ⼤量的数据拷⻉和内存分配。
  2.第⼆点， epoll 使⽤事件驱动的机制，内核⾥维护了⼀个链表来记录就绪事件，当某个
    socket 有事件发⽣时，通过回调函数内核会将其加⼊到这个就绪事件列表中，当⽤户调⽤
    epoll_wait() 函数时，只会返回有事件发⽣的⽂件描述符的个数，不需要像 select/poll 那
    样轮询扫描整个 socket 集合，⼤⼤提⾼了检测的效率。

  epoll 的⽅式即使监听的 Socket 数量越多的时候，效率不会⼤幅度降低，能够同时监听的
Socket 的数⽬也⾮常的多了，上限就为系统定义的进程打开的最⼤⽂件描述符个数。因⽽，
epoll 被称为解决 C10K 问题的利器


⾼性能⽹络模式：
   Reactor 模式也叫 Dispatcher 模式，我觉得这个名字更贴合该模式的含义，即
I/O 多路复⽤监听事件，收到事件后，根据事件类型分配（Dispatch）给某个进程/线程
Reactor 模式主要由 Reactor 和处理资源池这两个核⼼部分组成，它俩负责的事情如下：
 1.Reactor 负责监听和分发事件，事件类型包含连接事件、读写事件
 2.处理资源池负责处理事件，如 read -> 业务逻辑 -> send；

Reactor 模式是灵活多变的，可以应对不同的业务场景，灵活在于：
 1.Reactor 的数量可以只有⼀个，也可以有多个；
 2.处理资源池可以是单个进程 / 线程，也可以是多个进程 /线程；

将上⾯的两个因素排列组设⼀下，理论上就可以有 4 种⽅案选择:
  单 Reactor 单进程 / 线程；
  单 Reactor 多进程 / 线程；
  多 Reactor 单进程 / 线程；
  多 Reactor 多进程 / 线程；

   其中，「多 Reactor 单进程 / 线程」实现⽅案相⽐「单 Reactor 单进程 / 线程」⽅案，不仅
复杂⽽且也没有性能优势，因此实际中并没有应⽤。

剩下的 3 个⽅案都是⽐较经典的，且都有应⽤在实际的项⽬中：
  1.单 Reactor 单进程 / 线程；
  2.单 Reactor 多线程 / 进程
  3.多 Reactor 多进程 / 线程；

Reactor
  单 Reactor 单进程 / 线程结构：
  进程⾥有 Reactor、Acceptor、Handler 这三个对象：
  1.Reactor 对象的作⽤是监听和分发事件；
  2.Acceptor 对象的作⽤是获取连接；
  3.Handler 对象的作⽤是处理业务；

对象⾥的 select、accept、read、send 是系统调⽤函数，dispatch 和 「业务处理」是需要完的操作，其中 dispatch 是分发事件操作。

介绍下「单 Reactor 单进程」这个⽅案：
    1.Reactor 对象通过 select （IO 多路复⽤接⼝） 监听事件，收到事件后通过 dispatch 进⾏分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；
    2.如果是连接建⽴的事件，则交由 Acceptor 对象进⾏处理，Acceptor 对象会通过 accept⽅法 获取连接，并创建⼀个 Handler对象来处理后续的响应事件；
    3.如果不是连接建⽴事件， 则交由当前连接对应的 Handler 对象来进⾏响应；
    4.Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。

但是，这种⽅案存在 2 个缺点：
    1.第⼀个缺点，因为只有⼀个进程，⽆法充分利⽤ 多核 CPU 的性能；
    2.第⼆个缺点，Handler 对象在业务处理时，整个进程是⽆法处理其他连接的事件的，如果
      业务处理耗时⽐较⻓，那么就造成响应的延迟；

所以，单 Reactor 单进程的⽅案不适⽤计算机密集型的场景，只适⽤于业务处理⾮常快速的场景，Redis 是由 C 语⾔实现的，它采⽤的正是「单 Reactor 单进程」的⽅案，、
因为 Redis 业务处理主要是在内存中完成，操作的速度是很快的，性能瓶颈不在 CPU 上，所以 Redis 对于命令的处理是单进程的⽅案。

单 Reactor 多线程 / 多进程？
    1.Reactor 对象通过 select （IO 多路复⽤接⼝） 监听事件，收到事件后通过 dispatch 进⾏分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；
    2.如果是连接建⽴的事件，则交由 Acceptor 对象进⾏处理，Acceptor 对象会通过 accept⽅法 获取连接，并创建⼀个 Handler 对象来处理后续的响应事件；
    3.如果不是连接建⽴事件， 则交由当前连接对应的 Handler 对象来进⾏响应
上⾯的三个步骤和单 Reactor 单线程⽅案是⼀样的，接下来的步骤就开始不⼀样了：
   1.Handler 对象不再负责业务处理，只负责数据的接收和发送，Handler 对象通过 read 读取到数据后，会将数据发给⼦线程⾥的 Processor 对象进⾏业务处理；
   2.线程⾥的 Processor 对象就进⾏业务处理，处理完后，将结果发给主线程中的 Handler 对象，接着由 Handler 通过 send ⽅法将响应结果发送给 client；

    单 Reator 多线程的⽅案优势在于能够充分利⽤多核 CPU 的能，那既然引⼊多线程，那么⾃然就带来了多线程竞争资源的问题，⼦线程完成业务处理后，要把结果传递给主线程的 Reactor 进⾏发送，这⾥涉及共享
数据的竞争。避免多线程由于竞争共享资源⽽导致数据错乱的问题，就需要在操作共享资源前加上互斥
锁，以保证任意时间⾥只有⼀个线程在操作共享资源，待该线程操作完释放互斥锁后，其他
线程才有机会操作共享数据。事实上，单 Reactor 多进程相⽐单 Reactor 多线程实现起来很麻烦，主要因为要考虑⼦进程
 <-> ⽗进程的双向通信，并且⽗进程还得知道⼦进程要将数据发送给哪个客户端。
⽽多线程间可以共享数据，虽然要额外考虑并发问题，但是这远⽐进程间通信的复杂度低得
多，因此实际应⽤中也看不到单 Reactor 多进程的模式。
   另外，「单 Reactor」的模式还有个问题，因为⼀个 Reactor 对象承担所有事件的监听和响
应，⽽且只在主线程中运⾏，在⾯对瞬间⾼并发的场景时，容易成为性能的瓶颈的地⽅。

多Reactor多进程/线程⽅案详细说明如下：
   1.主线程中的 MainReactor 对象通过 select 监控连接建⽴事件，收到事件后通过 Acceptor对象中的 accept 获取连接，将新的连接分配给某个⼦线程；
   2.⼦线程中的 SubReactor 对象将 MainReactor 对象分配的连接加⼊ select 继续进⾏监听，并创建⼀个 Handler ⽤于处理连接的响应事件。
   3.如果有新的事件发⽣时，SubReactor 对象会调⽤当前连接对应的 Handler 对象来进⾏响应
   4.Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程

多 Reactor 多线程的⽅案虽然看起来复杂的，但是实际实现时⽐单 Reactor 多线程的⽅案要简单的多，原因如下：
   1.主线程和⼦线程分⼯明确，主线程只负责接收新连接，⼦线程负责完成后续的业务处理
   2.主线程和⼦线程的交互很简单，主线程只需要把新连接传给⼦线程，⼦线程⽆须返回数据，直接就可以在⼦线程将处理结果发送给客户端。
⼤名鼎鼎的两个开源软件 Netty 和 Memcache 都采⽤了「多 Reactor 多线程」的⽅案。采⽤了「多 Reactor 多进程」⽅案的开源软件是 Nginx，不过⽅案与标准的多 Reactor 多进
程有些差异。


Proactor？
   Reactor 是⾮阻塞同步⽹络模式，⽽ Proactor 是异步⽹络模式。

Reactor和Proactor的区别：
    1.Reactor 是⾮阻塞同步⽹络模式，感知的是就绪可读写事件。在每次感知到有事件发⽣（⽐如可读就绪事件）后，就需要应⽤进程主动调⽤ read ⽅法来完成数据的读取，也就
      是要应⽤进程主动将 socket 接收缓存中的数据读到应⽤进程内存中，这个过程是同步的，读取完数据后应⽤进程才能处理数据
    2.Proactor 是异步⽹络模式， 感知的是已完成的读写事件。在发起异步读写请求时，需要传⼊数据缓冲区的地址（⽤来存放结果数据）等信息，这样系统内核才可以⾃动帮我们把
      数据的读写⼯作完成，这⾥的读写⼯作全程由操作系统来做，并不需要像 Reactor 那样还需要应⽤进程主动发起 read/write 来读写数据，操作系统完成读写⼯作后，就会通知
      应⽤进程直接处理数据。

Proactor 模式的⼯作流程:
    1.Proactor Initiator 负责创建 Proactor 和 Handler 对象，并将 Proactor 和 Handler 都通过Asynchronous Operation Processor 注册到内核
    2.Asynchronous Operation Processor 负责处理注册请求，并处理 I/O 操作；
    3.Asynchronous Operation Processor 完成 I/O 操作后通知 Proactor；
    4.Proactor根据不同的事件类型回调不同的 Handler 进⾏业务处理；
    5.Handler完成业务处理；


Linux 命令:
    如何查看⽹络的性能指标？
     1.带宽，表示链路的最⼤传输速率，单位是 b/s （⽐特 / 秒），带宽越⼤，其传输能⼒就越强。
     2.延时，表示请求数据包发送后，收到对端响应，所需要的时间延迟。不同的场景有着不同的含义，⽐如可以表示建⽴ TCP 连接所需的时间延迟，或⼀个数据包往返所需的时间延迟。
     3.吞吐率，表示单位时间内成功传输的数据量，单位是 b/s（⽐特 / 秒）或者 B/s（字节 /秒），吞吐受带宽限制，带宽越⼤，吞吐率的上限才可能越⾼。
     4.PPS，全称是 Packet Per Second（包 / 秒），表示以⽹络包为单位的传输速率，⼀般⽤来评估系统对于⽹络的转发能⼒。

    当然，除了以上这四种基本的指标，还有⼀些其他常⽤的性能指标，⽐如：
     1.⽹络的可⽤性，表示⽹络能否正常通信；
     2.并发连接数，表示 TCP 连接数量；
     3.丢包率，表示所丢失数据包数量占所发送数据组的⽐率；
     4.重传率，表示重传⽹络包的⽐例；

当 socket 状态处于 Established 时:
    1.Recv-Q 表示 socket 缓冲区中还没有被应⽤程序读取的字节数
    2.Send-Q 表示 socket 缓冲区中还没有被远端主机确认的字节数；

⽽当 socket 状态处于 Listen 时：
    1.Recv-Q 表示全连接队列的⻓度；
    2.Send-Q 表示全连接队列的最⼤⻓度；

   在 TCP 三次握⼿过程中，当服务器收到客户端的 SYN 包后，内核会把该连接存储到半连接队列,然后再向客户端发送 SYN+ACK 包，接着客户端会返回 ACK，服务端收到第三次握⼿
的 ACK 后，内核会把连接从半连接队列移除，然后创建新的完全的连接，并将其增加到全连接队列 ，等待进程调⽤ accept() 函数时把连接取出来。
   也就说，全连接队列指的是服务器与客户端完了 TCP 三次握⼿后，还没有被 accept() 系
统调⽤取⾛连接的队列。

sar 命令当前⽹络的吞吐率和 PPS，⽤法是给 sar 增加 -n 参数就可以查看⽹络的统计信息，⽐如
   1.sar -n DEV，显示⽹⼝的统计数据
   2.sar -n EDEV，显示关于⽹络错误的统计数据
   3.sar -n TCP，显示 TCP 的统计数据


日志分析操作：
  别急着开始当我们要分析⽇志的时候，先⽤ ls -lh 命令查看⽇志⽂件的⼤⼩，如果⽇志⽂件⼤⼩⾮常⼤，最好不要在线上环境做。
当发现⽇志很⼤的时候，我们可以使⽤ scp 命令将⽂件传输到闲置的服务器再分析，scp 命
令使⽤⽅式如下图：
   scp access.log root@192.168.1.100:/home/

慎⽤ cat
   ⼤家都知道 cat 命令是⽤来查看⽂件内容的，但是⽇志⽂件数据量有多少，它就读多少，很显然不适⽤⼤⽂件。
对于⼤⽂件，我们应该养成好习惯，⽤ less 命令去读⽂件⾥的内容，因为 less 并不会加载整个⽂件，⽽是按需加载，先是输出⼀⼩⻚的内容，当你要往下看的时候，才会继续加载

不过，有时候我们想看⽇志最新部分的内容，可以使⽤ tail 命令，⽐如当你想查看倒数 5⾏的内容，你可以使⽤这样的命令:
  tail -n 4 access.log

PV分析
  PV 的全称叫 Page View，⽤户访问⼀个⻚⾯就是⼀次 PV，⽐如⼤多数博客平台，点击⼀次⻚⾯，阅读量就加 1，所以说 PV 的数量并不代表真实的⽤户数量，只是个点击量。
对于 nginx 的 acess.log ⽇志⽂件来说，分析 PV 还是⽐较容易的，既然⽇志⾥的内容是访问记录，那有多少条⽇志记录就有多少 PV。
我们直接使⽤ wc -l 命令，就可以查看整体的 PV 了。
  wc -l access.log

PV分组
   nginx 的 acess.log ⽇志⽂件有访问时间的信息，因此我们可以根据访问时间进⾏分组，⽐
如按天分组，查看每天的总 PV，这样可以得到更加直观的数据。要按时间分组，⾸先我们先「访问时间」过滤出来，这⾥可以使⽤ awk 命令来处理，awk 是
 ⼀个处理⽂本的利器。
   awk '{print $4}' access.log

上⾯的信息还包含了时分秒，如果只想显示年⽉⽇的信息，可以使⽤ awk 的 substr 函数，从第 2 个字符开始，截取 11 个字符
   awk '{print substr($4,2,11)}' access.log

接着，我们可以使⽤ sort 对⽇期进⾏排序，然后使⽤ uniq -c 进⾏统计，于是按天分组的 PV 就出来了
   awk '{print substr($4,2,11)}' access.log | sort | uniq -c
  使⽤ uniq -c uniq -c 命令前，先要进⾏ sort sort 排序，因为 uniq 去重的原理是⽐较相邻的
⾏，然后除去第⼆⾏和该⾏的后续副本，因此在使⽤ uniq 命令之前，请使⽤ sort 命令使所有
重复⾏相邻

UV 分析
  access.log ⽇志⾥虽然没有⽤户的身份信息，但是我们可以⽤「客户端 IP 地址」来近似统计UV

 awk '{print $1 }' access.log | sort | uniq | wc -l

 1.awk '{print $1}' access.log ，取⽇志的第 1 列内容，客户端的 IP 地址正是第 1列；
 2.sort ，对信息排序；
 3.uniq ，去除重复的记录；
 4.wc -l ，查看记录条数；


UV 分组
  假设我们按天来分组分析每天的 UV 数量，这种情况就稍微⽐较复杂，需要⽐较多的命令来实现
  awk '{print substr($4,2,11)} " " $1' access.log | sort | uniq

 具体分析：
   1.第⼀次 ack 是将第 4 列的⽇期和第 1 列的客户端 IP 地址过滤出来，并⽤空格拼接起来；
   2.然后 sort 对第⼀次 ack 输出的内容进⾏排序；
   3.接着⽤ uniq 去除重复的记录，也就说⽇期 +IP 相同的⾏就只保留⼀个；


分析 TOP3 的请求
    access.log ⽇志中，第 7 列是客户端请求的路径，先使⽤ awk 过滤出第 7 列的内容后，进
⾏ sort 排序，再⽤ uniq -c 去重并统计，然后再使⽤ sort -rn 对统计的结果排序，最
后使⽤ head -n 3 分析 TOP3 的请求，结果如下图：
    awk '{print $7}' access.log | sort | uniq -c |sort -rn | head -n 3